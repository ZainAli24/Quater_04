{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMy5UNUTjImnWmj7Iaeb4u4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZainAli24/Quater_04/blob/main/class2_OpenAI_Agents_SDK_Quiz_Prepration(Revision).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJpdsk7G2Nlg",
        "outputId": "8e11d418-ee7f-4f38-829b-159c4edd7d2f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/126.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.7/126.7 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/129.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.3/129.3 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/130.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/734.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m734.6/734.6 kB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/45.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -qU openai-agents"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "_VV7RXQL2_YH"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "GEMINI_API_KEY = userdata.get('GEMINI_API_KEY')"
      ],
      "metadata": {
        "id": "nqHekCy73Hsa"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from agents import Agent,Runner,AsyncOpenAI,OpenAIChatCompletionsModel\n",
        "from agents.run import RunConfig\n",
        "\n",
        "external_client = AsyncOpenAI(\n",
        "    api_key=GEMINI_API_KEY,\n",
        "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\",\n",
        ")\n",
        "\n",
        "model = OpenAIChatCompletionsModel(\n",
        "    model=\"gemini-2.0-flash\",\n",
        "    openai_client=external_client\n",
        ")\n",
        "\n",
        "model1 = OpenAIChatCompletionsModel(\n",
        "    model=\"gemini-1.5-flash\",\n",
        "    openai_client=external_client\n",
        ")\n",
        "\n",
        "config = RunConfig(\n",
        "    model=model,\n",
        "    model_provider=external_client,\n",
        "    tracing_disabled=True\n",
        ")\n"
      ],
      "metadata": {
        "id": "h_MpB3CK3RXG"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent = Agent(\n",
        "    name=\"Assistant\",\n",
        "    instructions=\"You are a helpful assistant.\",\n",
        "    model=model1\n",
        ")\n",
        "\n",
        "\n",
        "result = await Runner.run(\n",
        "    agent,\n",
        "    \"Add 3 in 67 \",\n",
        "    run_config=config,\n",
        ")\n",
        "\n",
        "print(result.new_items)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4jFHSAjd5IxG",
        "outputId": "cda11fcf-97a2-43aa-88cd-7be6398f2e25"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[MessageOutputItem(agent=Agent(name='Assistant', instructions='You are a helpful assistant.', prompt=None, handoff_description=None, handoffs=[], model=<agents.models.openai_chatcompletions.OpenAIChatCompletionsModel object at 0x7988494da150>, model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=None, truncation=None, max_tokens=None, reasoning=None, metadata=None, store=None, include_usage=None, extra_query=None, extra_body=None, extra_headers=None, extra_args=None), tools=[], mcp_servers=[], mcp_config={}, input_guardrails=[], output_guardrails=[], output_type=None, hooks=None, tool_use_behavior='run_llm_again', reset_tool_choice=True), raw_item=ResponseOutputMessage(id='__fake_id__', content=[ResponseOutputText(annotations=[], text='67 + 3 = 70\\n', type='output_text', logprobs=None)], role='assistant', status='completed', type='message'), type='message_output_item')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Asynchronous and Synchronous behaviour in run_sync and run_streamed:**\n",
        "\n",
        "Aapka sawal hai ke `run_streamed` method asynchronous hai ya synchronous, jabke yeh diya gaya hai ke `run_sync` method synchronous hai. Synchronous ka matlab hai ke agar ek agent multiple tasks kar raha hai, to woh ek sequence mein kaam karega aur jab tak pehla task complete nahi hota, woh dosre task pe nahi jayega. Jabke asynchronous mein, agar pehle task ka result abhi nahi aaya, to agent sath sath doosre tasks bhi karta rahega bina pehle task ke result ka wait kiye. Ab hum code ko dekhte hain ke `run_streamed` kaunsa approach use karta hai.\n",
        "\n",
        "### Code Analysis\n",
        "1. **run_sync Method**:\n",
        "   - Code mein `run_sync` method ko dekhein:\n",
        "     ```python\n",
        "     def run_sync(\n",
        "         self,\n",
        "         starting_agent: Agent[TContext],\n",
        "         input: str | list[TResponseInputItem],\n",
        "         **kwargs: Unpack[RunOptions[TContext]],\n",
        "     ) -> RunResult:\n",
        "         return asyncio.get_event_loop().run_until_complete(\n",
        "             self.run(\n",
        "                 starting_agent,\n",
        "                 input,\n",
        "                 context=kwargs.get(\"context\"),\n",
        "                 max_turns=kwargs.get(\"max_turns\", DEFAULT_MAX_TURNS),\n",
        "                 hooks=kwargs.get(\"hooks\"),\n",
        "                 run_config=kwargs.get(\"run_config\"),\n",
        "                 previous_response_id=kwargs.get(\"previous_response_id\"),\n",
        "             )\n",
        "         )\n",
        "     ```\n",
        "   - Yeh method `self.run` (jo ke asynchronous hai) ko `asyncio.get_event_loop().run_until_complete` ke zariye call karta hai. Yeh ek synchronous wrapper hai jo async `run` method ko synchronous banata hai. Yani, yeh pura operation complete hone tak wait karta hai, jo synchronous behavior hai. Agar multiple tasks hain, to yeh ek ke baad ek execute honge.\n",
        "\n",
        "2. **run_streamed Method**:\n",
        "   - Ab `run_streamed` method ko dekhte hain:\n",
        "     ```python\n",
        "     def run_streamed(\n",
        "         self,\n",
        "         starting_agent: Agent[TContext],\n",
        "         input: str | list[TResponseInputItem],\n",
        "         **kwargs: Unpack[RunOptions[TContext]],\n",
        "     ) -> RunResultStreaming:\n",
        "         ...\n",
        "         streamed_result._run_impl_task = asyncio.create_task(\n",
        "             self._start_streaming(\n",
        "                 starting_input=input,\n",
        "                 streamed_result=streamed_result,\n",
        "                 starting_agent=starting_agent,\n",
        "                 max_turns=max_turns,\n",
        "                 hooks=hooks,\n",
        "                 context_wrapper=context_wrapper,\n",
        "                 run_config=run_config,\n",
        "                 previous_response_id=previous_response_id,\n",
        "             )\n",
        "         )\n",
        "         return streamed_result\n",
        "     ```\n",
        "   - Yeh method ek `RunResultStreaming` object return karta hai, aur actual agent loop ko `_start_streaming` method ke zariye background mein chalata hai using `asyncio.create_task`. Yeh task asynchronous hai, kyunki `asyncio.create_task` ek coroutine ko background mein schedule karta hai aur control immediately wapas de deta hai bina task ke complete hone ka wait kiye.\n",
        "   - `_start_streaming` method bhi asynchronous hai, kyunki isme `async def` use hua hai aur yeh `await` statements ke sath kaam karta hai, jaise:\n",
        "     ```python\n",
        "     async def _start_streaming(\n",
        "         cls,\n",
        "         starting_input: str | list[TResponseInputItem],\n",
        "         streamed_result: RunResultStreaming,\n",
        "         starting_agent: Agent[TContext],\n",
        "         max_turns: int,\n",
        "         hooks: RunHooks[TContext],\n",
        "         context_wrapper: RunContextWrapper[TContext],\n",
        "         run_config: RunConfig,\n",
        "         previous_response_id: str | None,\n",
        "     ):\n",
        "         ...\n",
        "         async for event in model.stream_response(...):\n",
        "             streamed_result._event_queue.put_nowait(RawResponsesStreamEvent(data=event))\n",
        "     ```\n",
        "   - Yeh method events ko stream karta hai using an async generator (`async for`), jo ke asynchronous behavior hai. Yani, yeh events ko generate karta rahta hai jab tak woh available hote hain, aur sath hi sath doosre tasks (jaise input/output guardrails) ko bhi background mein handle karta hai.\n",
        "\n",
        "3. **Asynchronous Behavior in run_streamed**:\n",
        "   - `run_streamed` mein `_run_impl_task` ko `asyncio.create_task` ke sath background mein chalaya jata hai, jo ke asynchronous execution ko indicate karta hai.\n",
        "   - Input guardrails bhi background mein chalte hain:\n",
        "     ```python\n",
        "     streamed_result._input_guardrails_task = asyncio.create_task(\n",
        "         cls._run_input_guardrails_with_queue(...)\n",
        "     )\n",
        "     ```\n",
        "   - Yeh dikhata hai ke guardrails aur streaming tasks sath sath chal sakte hain, jo asynchronous behavior hai.\n",
        "   - `stream_response` method bhi async hai aur events ko stream karta hai, jo ke is baat ko reinforce karta hai ke `run_streamed` non-blocking hai aur multiple tasks concurrently handle kar sakta hai.\n",
        "\n",
        "4. **Synchronous vs Asynchronous**:\n",
        "   - **Synchronous (`run_sync`)**: Yeh method pura operation complete hone tak wait karta hai, yani ek task ke baad doosra task shuru hota hai. Yeh sequential execution hai.\n",
        "   - **Asynchronous (`run_streamed`)**: Yeh method tasks ko background mein schedule karta hai (`asyncio.create_task`) aur events ko stream karta hai bina pura operation complete hone ke wait kiye. Yeh allow karta hai ke agent sath sath multiple tasks handle kare, jaise guardrails ko run karna aur events ko stream karna, bina pehle task ke result ka wait kiye.\n",
        "\n",
        "### Nateeja\n",
        "`run_streamed` method **asynchronous** hai. Yeh asynchronous scenario ko use karta hai, kyunki:\n",
        "- Yeh tasks ko background mein schedule karta hai using `asyncio.create_task`.\n",
        "- Yeh events ko asynchronously stream karta hai using `async for`.\n",
        "- Yeh guardrails aur streaming tasks ko sath sath chalane deta hai, jo ke asynchronous behavior hai.\n",
        "\n",
        "Jabke `run_sync` synchronous hai, kyunki yeh pura operation complete hone tak wait karta hai aur tasks ko sequence mein execute karta hai.\n",
        "\n",
        "Agar aapko mazeed wazahat chahiye ya koi specific part ke bare mein poochna hai, to mujhe batayein!"
      ],
      "metadata": {
        "id": "PKheo8R_C-jV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agent = Agent(\n",
        "    name=\"Assistant\",\n",
        "    instructions=\"You are a helpful assistant.\",\n",
        "    model=model1\n",
        ")\n",
        "\n",
        "\n",
        "result = Runner.run_streamed(\n",
        "    agent,\n",
        "    \"Hi, My Name is Zain\",\n",
        "    run_config=config,\n",
        ")\n",
        "\n",
        "print(result.to_input_list())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Df0STouJcni",
        "outputId": "cd7b7442-ccc3-4909-e6b7-cc6e935c0dfe"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'content': 'Hi, My Name is Zain', 'role': 'user'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "----------\n",
        "# ***) RunResult and RunResultStreaming properties and methods working:**"
      ],
      "metadata": {
        "id": "WFPr4dk-9cO3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. to_input_list():** input list LLM aur user ki conversation jo ik run mien hoti hai wo deta hai yani user ne sawal pocha aur llm ne jawab diya :\n",
        "\n",
        "### Example:\n",
        "```python\n",
        "[{'content': 'Add 3 in 67 ', 'role': 'user'}, {'id': '__fake_id__', 'content': [{'annotations': [], 'text': '67 + 3 = 70\\n', 'type': 'output_text'}], 'role': 'assistant', 'status': 'completed', 'type': 'message'}]\n",
        "```"
      ],
      "metadata": {
        "id": "jT2X_8Urgr40"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "------------\n",
        "## **2. new_items ka result = MessageOutputItem:**\n",
        "\n",
        "Aap ne mujhe ek code snippet diya hai jo ek `MessageOutputItem` ko represent karta hai. Yeh ek list hai jisme sirf ek item hai, aur yeh item `MessageOutputItem` class ka instance hai. Isme do main parts hain: ek `agent` aur ek `raw_item`. Mein aapko har ek cheez ko detail mein samjhaonga ke yeh kya hai aur kya kaam karti hai.\n",
        "\n",
        "---\n",
        "\n",
        "### **Pura Structure**\n",
        "Yeh code ek list hai:\n",
        "\n",
        "```python\n",
        "[\n",
        "    MessageOutputItem(\n",
        "        agent=Agent(...),\n",
        "        raw_item=ResponseOutputMessage(...),\n",
        "        type='message_output_item'\n",
        "    )\n",
        "]\n",
        "```\n",
        "\n",
        "- **Kya hai?**: Yeh ek list hai jisme ek `MessageOutputItem` object hai.\n",
        "- **Kaam**: Yeh agent ke output ko organize karne ke liye hai, jisme agent ki configuration aur uska generated message shamil hota hai.\n",
        "\n",
        "Ab iske har part ko detail mein dekhte hain.\n",
        "\n",
        "---\n",
        "\n",
        "### **1. `MessageOutputItem`**\n",
        "- **Kya hai?**: Yeh ek class ka instance hai jo agent ke dwara produce kiye gaye message ko hold karta hai.\n",
        "- **Kaam**: Yeh ek structure provide karta hai jisme agent ki details aur uska output message ek sath store hota hai.\n",
        "\n",
        "#### **Attributes:**\n",
        "- **`agent`**: Ek `Agent` object jo message generate karne wala AI agent hai.\n",
        "- **`raw_item`**: Ek `ResponseOutputMessage` object jo actual message ko contain karta hai.\n",
        "- **`type='message_output_item'`**: Yeh specify karta hai ke yeh ek message output item hai.\n",
        "\n",
        "---\n",
        "\n",
        "### **2. `agent`**\n",
        "- **Kya hai?**: Yeh `Agent` class ka instance hai jo ek AI agent ko represent karta hai. Is agent ko configure kiya gaya hai taake woh specific tasks perform kar sake, jaise arithmetic calculation ya user ke sawalon ka jawab dena.\n",
        "- **Kaam**: Yeh agent user ke input ke base par responses generate karta hai. Is case mein, agent ka name \"Assistant\" hai aur uski instructions \"You are a helpful assistant.\" hain, jo batata hai ke yeh user ki madad karne ke liye banaya gaya hai.\n",
        "\n",
        "#### **Agent ke Attributes:**\n",
        "- **`name='Assistant'`**  \n",
        "  - **Kya hai?**: Agent ka name.  \n",
        "  - **Kaam**: Yeh identify karta hai ke agent kaun hai. Yahan par \"Assistant\" ek simple identifier hai.\n",
        "\n",
        "- **`instructions='You are a helpful assistant.'`**  \n",
        "  - **Kya hai?**: Agent ko batane wali instructions.  \n",
        "  - **Kaam**: Yeh agent ko uska role ya behavior define karta hai, yani yeh user ki madad karne wala assistant hai.\n",
        "\n",
        "- **`handoff_description=None`**  \n",
        "  - **Kya hai?**: Ek description jo shayad doosre agent ko control handoff karne ke liye hoti hai.  \n",
        "  - **Kaam**: Yahan `None` hai, yani is feature ka use nahi ho raha.\n",
        "\n",
        "- **`handoffs=[]`**  \n",
        "  - **Kya hai?**: Doosre agents ki list jinko control handoff kiya ja sakta hai.  \n",
        "  - **Kaam**: Khali list hai, yani koi handoff nahi hai.\n",
        "\n",
        "- **`model=<agents.models.openai_chatcompletions.OpenAIChatCompletionsModel object at 0x00000202EEBB9A90>`**  \n",
        "  - **Kya hai?**: Yeh ek OpenAI chat completion model ka instance hai jo agent responses generate karne ke liye use karta hai.  \n",
        "  - **Kaam**: Yeh agent ka \"brain\" hai jo text ya answers produce karta hai.\n",
        "\n",
        "- **`model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=None, truncation=None, max_tokens=None, reasoning=None, metadata=None, store=None, include_usage=None, extra_query=None, extra_body=None, extra_headers=None)`**  \n",
        "  - **Kya hai?**: Model ke liye settings ka ek object.  \n",
        "  - **Kaam**: Yeh model ke behavior ko control karta hai (e.g., randomness, token limit), lekin yahan sab `None` hain, jo shayad default settings ko indicate karta hai.\n",
        "\n",
        "- **`tools=[]`**  \n",
        "  - **Kya hai?**: External tools ki list jo agent use kar sakta hai.  \n",
        "  - **Kaam**: Khali hai, yani koi tools nahi hain; agent sirf model ke through kaam karta hai.\n",
        "\n",
        "- **`mcp_servers=[]`**  \n",
        "  - **Kya hai?**: Shayad model control panel servers ki list.  \n",
        "  - **Kaam**: Khali hai, yani is feature ka use nahi ho raha.\n",
        "\n",
        "- **`mcp_config={}`**  \n",
        "  - **Kya hai?**: Model control panel ka configuration dictionary.  \n",
        "  - **Kaam**: Khali hai, yani koi specific config nahi.\n",
        "\n",
        "- **`input_guardrails=[]`**  \n",
        "  - **Kya hai?**: Input ko validate ya filter karne wale rules ki list.  \n",
        "  - **Kaam**: Khali hai, yani koi input checking nahi.\n",
        "\n",
        "- **`output_guardrails=[]`**  \n",
        "  - **Kya hai?**: Output ko validate ya filter karne wale rules ki list.  \n",
        "  - **Kaam**: Khali hai, yani koi output checking nahi.\n",
        "\n",
        "- **`output_type=None`**  \n",
        "  - **Kya hai?**: Expected output ka type.  \n",
        "  - **Kaam**: `None` hai, yani koi specific type define nahi kiya gaya.\n",
        "\n",
        "- **`hooks=None`**  \n",
        "  - **Kya hai?**: Custom behavior ya callbacks ke liye hooks.  \n",
        "  - **Kaam**: `None` hai, yani koi hooks nahi.\n",
        "\n",
        "- **`tool_use_behavior='run_llm_again'`**  \n",
        "  - **Kya hai?**: Tool use hone par kya karna hai, yahan language model ko dobara run karna hai.  \n",
        "  - **Kaam**: Chunki koi tools nahi hain, yeh relevant nahi hai.\n",
        "\n",
        "- **`reset_tool_choice=True`**  \n",
        "  - **Kya hai?**: Tool choice ko reset karne ka option.  \n",
        "  - **Kaam**: Har use ke baad tool choice reset hota hai, lekin koi tools nahi hain to yeh bhi relevant nahi.\n",
        "\n",
        "---\n",
        "\n",
        "### **3. `raw_item`**\n",
        "- **Kya hai?**: Yeh `ResponseOutputMessage` class ka instance hai jo agent ke dwara generate kiya gaya actual message hai.\n",
        "- **Kaam**: Yeh message ki details (content, role, status) ko store karta hai.\n",
        "\n",
        "#### **ResponseOutputMessage ke Attributes:**\n",
        "- **`id='__fake_id__'`**  \n",
        "  - **Kya hai?**: Message ka ID.  \n",
        "  - **Kaam**: Yeh ek placeholder ID hai jo message ko identify karta hai.\n",
        "\n",
        "- **`content=[ResponseOutputText(annotations=[], text='67 + 3 = 70\\n', type='output_text')]`**  \n",
        "  - **Kya hai?**: Message ka content, jo ek list mein ek `ResponseOutputText` object hai.  \n",
        "  - **Kaam**: Yeh actual text \"67 + 3 = 70\" ko hold karta hai.  \n",
        "    - **`annotations=[]`**: Koi additional notes ya metadata nahi.  \n",
        "    - **`text='67 + 3 = 70\\n'`**: Yeh calculation ka result hai jo agent ne diya.  \n",
        "    - **`type='output_text'`**: Yeh batata hai ke content text type ka hai.\n",
        "\n",
        "- **`role='assistant'`**  \n",
        "  - **Kya hai?**: Message kis role se hai.  \n",
        "  - **Kaam**: \"assistant\" indicate karta hai ke yeh agent ka response hai.\n",
        "\n",
        "- **`status='completed'`**  \n",
        "  - **Kya hai?**: Message ka status.  \n",
        "  - **Kaam**: \"completed\" batata hai ke message fully generate ho chuka hai.\n",
        "\n",
        "- **`type='message'`**  \n",
        "  - **Kya hai?**: Yeh item ka type.  \n",
        "  - **Kaam**: \"message\" specify karta hai ke yeh ek message hai.\n",
        "\n",
        "---\n",
        "\n",
        "### **Overall Kaam**\n",
        "Yeh pura structure ek AI agent \"Assistant\" ke dwara generate kiye gaye message ko represent karta hai. Agent ek basic configuration ke sath set kiya gaya hai (koi tools, guardrails, ya hooks nahi). Isne ek simple arithmetic calculation \"67 + 3 = 70\" perform kiya aur uska result text message ke roop mein diya, jo ke sahi hai.\n",
        "\n",
        "#### **Summary:**\n",
        "- **Agent**: Ek \"Assistant\" jo OpenAI model use karta hai aur user ki madad ke liye banaya gaya hai.\n",
        "- **Configuration**: Basic settings, koi extra features nahi.\n",
        "- **Output**: Ek completed message jisme \"67 + 3 = 70\" likha hai.\n"
      ],
      "metadata": {
        "id": "Eb5TWCxV6VfL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---------\n",
        "## **3. ModelSettings:**\n",
        "\n",
        "## **ModelSettings Overview**\n",
        "`ModelSettings` ek configuration hai jo batati hai ke AI model (jaise chatbot) kaise behave karega. Yeh ek class hai jismein 15 attributes hain, aur inki default value `None` hoti hai, yani agar aap kuch set nahi karte to model apni default setting use karta hai. Neeche har attribute ko detail mein samjha raha hoon.\n",
        "\n",
        "---\n",
        "\n",
        "### **1. `temperature`**\n",
        "- **Kya Karta Hai**: Yeh model ke jawab ki \"creativity\" ko control karta hai. Zyada temperature se jawab creative aur random hote hain, kam temperature se jawab seedhe aur focused hote hain.\n",
        "- **Code Example**:\n",
        "  ```python\n",
        "  model_settings = ModelSettings(temperature=0.8)\n",
        "  ```\n",
        "- **Use Case**:\n",
        "  - **Situation**: Aap ek kahani likhne wala bot bana rahe hain.\n",
        "  - **Setting**: `temperature=0.8` set karen taake bot naye aur dilchasp ideas de.\n",
        "  - **Misal**: \"Ek jungle mein ek anokha janwar tha...\" (creative output).\n",
        "- **Simple Bhasha Mein**: Temperature model ko batata hai ke kitna \"mazedaar\" ya \"serious\" hona hai.\n",
        "\n",
        "---\n",
        "\n",
        "### **2. `top_p`**\n",
        "- **Meaning**: Yeh model ko batata hai ke kitne \"probable\" shabd ya jawab chunne hain. Iska value 0.0 se 1.0 tak hota hai.\n",
        "- **Value Zyada Hone Ka Effect (e.g., `top_p=0.95`)**:\n",
        "  - Model zyada options se jawab chunta hai, isliye jawab mein variety aati hai.\n",
        "  - Example: \"Mausam kaisa hai?\" ka jawab ho sakta hai: \"Mausam shandaar hai!\" ya \"Din sundar hai.\"\n",
        "- **Value Kam Hone Ka Effect (e.g., `top_p=0.1`)**:\n",
        "  - Model sirf sabse zyada likely jawab deta hai, isliye jawab predictable hote hain.\n",
        "  - Example: \"Mausam acha hai.\" (Har baar yahi jawab).\n",
        "- **Code Example**:\n",
        "  ```python\n",
        "  model_settings = ModelSettings(top_p=0.95)  # Zyada variety\n",
        "  # Ya\n",
        "  model_settings = ModelSettings(top_p=0.1)   # Kam variety\n",
        "  ```\n",
        "- **Asaan Samajh**: Top_p model ko batata hai ke kitne jawabon ke \"menu\" se chunna hai. Zyada top_p = zyada choices, kam top_p = ek hi jawab.\n",
        "\n",
        "---\n",
        "\n",
        "### **3. `frequency_penalty`**\n",
        "- **Meaning**: Yeh model ko same shabd ya sentences baar-baar bolne se rokta hai. Value usually 0.0 se 2.0 tak hoti hai.\n",
        "- **Value Zyada Hone Ka Effect (e.g., `frequency_penalty=1.0`)**:\n",
        "  - Model naye shabd use karta hai aur repeat bilkul nahi karta.\n",
        "  - Example: \"Mausam kaisa hai?\" ka jawab: \"Mausam acha hai.\" Doosra jawab: \"Din shandaar hai.\" (repeat nahi).\n",
        "- **Value Kam Hone Ka Effect (e.g., `frequency_penalty=0.0`)**:\n",
        "  - Model shabd ya sentences repeat kar sakta hai.\n",
        "  - Example: \"Mausam acha hai.\" Doosra jawab: \"Mausam acha hai.\" (same jawab).\n",
        "- **Code Example**:\n",
        "  ```python\n",
        "  model_settings = ModelSettings(frequency_penalty=1.0)  # Kam repeat\n",
        "  # Ya\n",
        "  model_settings = ModelSettings(frequency_penalty=0.0)  # Zyada repeat\n",
        "  ```\n",
        "- **Asaan Samajh**: Frequency_penalty model ko kehta hai ke \"nayi baat bol, purani mat dohra\". Zyada penalty = naye shabd, kam penalty = repeat allowed.\n",
        "\n",
        "---\n",
        "\n",
        "### **4. `presence_penalty`**\n",
        "- **Meaning**: Yeh model ko input ya pehle ke jawab ke shabd copy karne se rokta hai. Value usually 0.0 se 2.0 tak hoti hai.\n",
        "- **Value Zyada Hone Ka Effect (e.g., `presence_penalty=1.0`)**:\n",
        "  - Model naye shabd ya ideas deta hai, input ko copy nahi karta.\n",
        "  - Example: Input: \"Mausam acha hai.\" Jawab: \"Din sundar lag raha hai.\" (input ke shabd nahi).\n",
        "- **Value Kam Hone Ka Effect (e.g., `presence_penalty=0.0`)**:\n",
        "  - Model input ke shabd ya ideas copy kar sakta hai.\n",
        "  - Example: Input: \"Mausam acha hai.\" Jawab: \"Haan, mausam acha hai.\" (copy kiya).\n",
        "- **Code Example**:\n",
        "  ```python\n",
        "  model_settings = ModelSettings(presence_penalty=1.0)  # Naye ideas\n",
        "  # Ya\n",
        "  model_settings = ModelSettings(presence_penalty=0.0)  # Copy allowed\n",
        "  ```\n",
        "- **Asaan Samajh**: Presence_penalty kehta hai \"user ki baat ko copy mat kar, kuch naya bol\". Zyada penalty = naya jawab, kam penalty = copy kar sakta hai.\n",
        "\n",
        "---\n",
        "\n",
        "### **5. `tool_choice`**\n",
        "- **Meaning**: Yeh batata hai ke model ko kaunsa external tool (jaise calculator) use karna hai. Value ek string ya specific tool name hota hai.\n",
        "- **Value Zyada (Set Kiya, e.g., `tool_choice=\"calculator\")`**:\n",
        "  - Model specific tool use karta hai aur usi se jawab deta hai.\n",
        "  - Example: \"2 + 2 kitna hai?\" Jawab: \"4\" (calculator se).\n",
        "- **Value Kam (Not Set, e.g., `tool_choice=None`)**:\n",
        "  - Model apne text knowledge se jawab deta hai, tool nahi use karta.\n",
        "  - Example: \"2 + 2 kitna hai?\" Jawab: \"Char\" (text-based).\n",
        "- **Code Example**:\n",
        "  ```python\n",
        "  model_settings = ModelSettings(tool_choice=\"calculator\")  # Tool use karen\n",
        "  # Ya\n",
        "  model_settings = ModelSettings(tool_choice=None)         # No tool\n",
        "  ```\n",
        "- **Asaan Samajh**: Tool_choice model ko batata hai ke koi bahari \"machine\" use karni hai ya nahi. Set kiya = tool use, nahi set = apna dimag use.\n",
        "\n",
        "---\n",
        "\n",
        "### **6. `parallel_tool_calls`**\n",
        "- **Meaning**: Yeh model ko ek saath multiple tools use karne deta hai. Value `True` ya `False` hoti hai.\n",
        "- **Value Zyada (e.g., `parallel_tool_calls=True`)**:\n",
        "  - Model ek saath kai tools se data le sakta hai, jisse kaam jaldi hota hai.\n",
        "  - Example: \"Mausam aur traffic kaisa hai?\" Jawab: \"Mausam 25°C hai, traffic kam hai.\" (dono tools ek saath).\n",
        "- **Value Kam (e.g., `parallel_tool_calls=False`)**:\n",
        "  - Model ek-ek karke tools use karta hai, jisse time lagta hai.\n",
        "  - Example: Pehle mausam, phir traffic ka jawab alag-alag.\n",
        "- **Code Example**:\n",
        "  ```python\n",
        "  model_settings = ModelSettings(parallel_tool_calls=True)   # Ek saath tools\n",
        "  # Ya\n",
        "  model_settings = ModelSettings(parallel_tool_calls=False)  # Ek-ek tool\n",
        "  ```\n",
        "- **Asaan Samajh**: Parallel_tool_calls model ko kehta hai ke \"ek saath do kaam kar\" ya \"ek ke baad doosra\". True = jaldi kaam, False = dheema kaam.\n",
        "\n",
        "---\n",
        "\n",
        "### **7. `truncation`**\n",
        "- **Meaning**: Yeh input ya jawab ko chhota karne ka rule set karta hai. Value ek number ya specific limit hoti hai.\n",
        "- **Value Zyada (e.g., `truncation=50`)**:\n",
        "  - Jawab ya input ko strictly 50 characters tak chhota karta hai.\n",
        "  - Example: \"Mausam kaisa hai?\" Jawab: \"Mausam acha hai, 25°C.\" (50 chars ke andar).\n",
        "- **Value Kam (e.g., `truncation=10`)**:\n",
        "  - Jawab bahut chhota ho jata hai, shayad incomplete.\n",
        "  - Example: \"Mausam acha...\" (sirf 10 chars).\n",
        "- **Code Example**:\n",
        "  ```python\n",
        "  model_settings = ModelSettings(truncation=50)  # 50 chars tak\n",
        "  # Ya\n",
        "  model_settings = ModelSettings(truncation=10)  # 10 chars tak\n",
        "  ```\n",
        "- **Asaan Samajh**: Truncation model ko kehta hai ke \"itna hi bol, zyada nahi\". Zyada value = thoda lamba jawab, kam value = chhota jawab.\n",
        "\n",
        "---\n",
        "\n",
        "### **8. `max_tokens`**\n",
        "- **Meaning**: Yeh jawab mein maximum shabd ya \"tokens\" ki limit set karta hai. Value ek number hoti hai.\n",
        "- **Value Zyada (e.g., `max_tokens=100`)**:\n",
        "  - Model lamba jawab de sakta hai, lekin 100 shabd se zyada nahi.\n",
        "  - Example: \"Mausam kaisa hai?\" Jawab: \"Mausam bahut acha hai, 25°C hai, aur thodi hawa bhi chal rahi hai.\"\n",
        "- **Value Kam (e.g., `max_tokens=10`)**:\n",
        "  - Jawab chhota aur seedha hota hai.\n",
        "  - Example: \"Mausam acha hai.\"\n",
        "- **Code Example**:\n",
        "  ```python\n",
        "  model_settings = ModelSettings(max_tokens=100)  # Lamba jawab\n",
        "  # Ya\n",
        "  model_settings = ModelSettings(max_tokens=10)   # Chhota jawab\n",
        "  ```\n",
        "- **Asaan Samajh**: Max_tokens model ko kehta hai ke \"itne shabd tak bol\". Zyada tokens = lamba, kam tokens = chhota.\n",
        "\n",
        "---\n",
        "\n",
        "### **9. `reasoning`**\n",
        "- **Meaning**: Yeh model ko step-by-step sochne ya explanation dene ke liye kehta hai. Value `True` ya `False` hoti hai.\n",
        "- **Value Zyada (e.g., `reasoning=True`)**:\n",
        "  - Model jawab ke saath soch ka process batata hai.\n",
        "  - Example: \"2 + 2 kitna hai?\" Jawab: \"Pehle 2 lo, phir 2 jodo, to 4 banta hai.\"\n",
        "- **Value Kam (e.g., `reasoning=False`)**:\n",
        "  - Model sirf final jawab deta hai.\n",
        "  - Example: \"2 + 2 = 4.\"\n",
        "- **Code Example**:\n",
        "  ```python\n",
        "  model_settings = ModelSettings(reasoning=True)   # Steps ke saath\n",
        "  # Ya\n",
        "  model_settings = ModelSettings(reasoning=False)  # Sirf jawab\n",
        "  ```\n",
        "- **Asaan Samajh**: Reasoning model ko kehta hai ke \"batana kaise kiya\". True = steps bataye, False = sirf result.\n",
        "\n",
        "---\n",
        "\n",
        "### **10. `metadata`**\n",
        "- **Meaning**: Yeh extra information model ya system ko deta hai, jaise user ka naam ya session ID. Value ek dictionary hoti hai.\n",
        "- **Value Zyada (e.g., `metadata={\"user\": \"Rahul\", \"session\": \"123\"}`)**:\n",
        "  - Model ko context milta hai, aur jawab personalized ho sakta hai.\n",
        "  - Example: \"Rahul, mausam acha hai.\" (user ke naam ke saath).\n",
        "- **Value Kam (e.g., `metadata=None`)**:\n",
        "  - Model bina kisi extra info ke jawab deta hai.\n",
        "  - Example: \"Mausam acha hai.\" (generic jawab).\n",
        "- **Code Example**:\n",
        "  ```python\n",
        "  model_settings = ModelSettings(metadata={\"user\": \"Rahul\"})  # Personal info\n",
        "  # Ya\n",
        "  model_settings = ModelSettings(metadata=None)              # No info\n",
        "  ```\n",
        "- **Asaan Samajh**: Metadata model ko kehta hai ke \"yeh user kaun hai\". Zyada info = personal jawab, kam info = simple jawab.\n",
        "\n",
        "---\n",
        "\n",
        "### **11. `store`**\n",
        "- **Meaning**: Yeh batata hai ke baat-cheet ko save karna hai ya nahi. Value `True` ya `False` hoti hai.\n",
        "- **Value Zyada (e.g., `store=True`)**:\n",
        "  - Har sawal aur jawab save ho jata hai.\n",
        "  - Example: \"Mausam kaisa hai?\" aur jawab database mein save ho jaye.\n",
        "- **Value Kam (e.g., `store=False`)**:\n",
        "  - Kuch bhi save nahi hota.\n",
        "  - Example: Jawab deta hai aur bhool jata hai.\n",
        "- **Code Example**:\n",
        "  ```python\n",
        "  model_settings = ModelSettings(store=True)   # Save karen\n",
        "  # Ya\n",
        "  model_settings = ModelSettings(store=False)  # Save nahi\n",
        "  ```\n",
        "- **Asaan Samajh**: Store model ko kehta hai ke \"baat yaad rakho\". True = save karta hai, False = bhool jata hai.\n",
        "\n",
        "---\n",
        "\n",
        "### **12. `include_usage`**\n",
        "- **Meaning**: Yeh batata hai ke jawab ke saath resource usage (jaise kitne shabd use hue) dikhana hai ya nahi. Value `True` ya `False` hoti hai.\n",
        "- **Value Zyada (e.g., `include_usage=True`)**:\n",
        "  - Jawab ke saath usage stats milte hain.\n",
        "  - Example: \"Mausam acha hai. [10 tokens use hue].\"\n",
        "- **Value Kam (e.g., `include_usage=False`)**:\n",
        "  - Sirf jawab milta hai, stats nahi.\n",
        "  - Example: \"Mausam acha hai.\"\n",
        "- **Code Example**:\n",
        "  ```python\n",
        "  model_settings = ModelSettings(include_usage=True)   # Stats dikhaye\n",
        "  # Ya\n",
        "  model_settings = ModelSettings(include_usage=False)  # Stats nahi\n",
        "  ```\n",
        "- **Asaan Samajh**: Include_usage model ko kehta hai ke \"kitna kaam kiya, woh bhi bata\". True = stats deta hai, False = sirf jawab.\n",
        "\n",
        "---\n",
        "\n",
        "### **13. `extra_query`**\n",
        "- **Meaning**: Yeh API call ke liye extra instructions ya parameters deta hai. Value ek dictionary hoti hai.\n",
        "- **Value Zyada (e.g., `extra_query={\"speed\": \"fast\"}`)**:\n",
        "  - API ko specific instructions milti hain, jaise jaldi jawab do.\n",
        "  - Example: Jawab turant milta hai.\n",
        "- **Value Kam (e.g., `extra_query=None`)**:\n",
        "  - API normal tareeke se chalta hai.\n",
        "  - Example: Jawab normal speed se aata hai.\n",
        "- **Code Example**:\n",
        "  ```python\n",
        "  model_settings = ModelSettings(extra_query={\"speed\": \"fast\"})  # Fast mode\n",
        "  # Ya\n",
        "  model_settings = ModelSettings(extra_query=None)              # Normal mode\n",
        "  ```\n",
        "- **Asaan Samajh**: Extra_query model ko kehta hai ke \"API ko yeh extra baat bol\". Zyada query = special mode, kam query = normal mode.\n",
        "\n",
        "---\n",
        "\n",
        "### **14. `extra_body`**\n",
        "- **Meaning**: Yeh API request ke body mein extra data jodta hai. Value ek dictionary hoti hai.\n",
        "- **Value Zyada (e.g., `extra_body={\"priority\": \"high\"}`)**:\n",
        "  - API ko extra info milti hai, jaise urgent request.\n",
        "  - Example: Jawab pehle process hota hai.\n",
        "- **Value Kam (e.g., `extra_body=None`)**:\n",
        "  - API normal request ke saath chalta hai.\n",
        "  - Example: Jawab normal queue mein aata hai.\n",
        "- **Code Example**:\n",
        "  ```python\n",
        "  model_settings = ModelSettings(extra_body={\"priority\": \"high\"})  # Urgent\n",
        "  # Ya\n",
        "  model_settings = ModelSettings(extra_body=None)                 # Normal\n",
        "  ```\n",
        "- **Asaan Samajh**: Extra_body model ko kehta hai ke \"request ke saath yeh jod do\". Zyada body = special request, kam body = simple request.\n",
        "\n",
        "---\n",
        "\n",
        "### **15. `extra_headers`**\n",
        "- **Meaning**: Yeh API call ke liye extra HTTP headers (jaise security keys) deta hai. Value ek dictionary hoti hai.\n",
        "- **Value Zyada (e.g., `extra_headers={\"api_key\": \"secret\"}`)**:\n",
        "  - API secure aur verified hoti hai.\n",
        "  - Example: Sirf sahi key wale jawab dete hain.\n",
        "- **Value Kam (e.g., `extra_headers=None`)**:\n",
        "  - API bina extra security ke chalta hai.\n",
        "  - Example: Normal API call.\n",
        "- **Code Example**:\n",
        "  ```python\n",
        "  model_settings = ModelSettings(extra_headers={\"api_key\": \"secret\"})  # Secure\n",
        "  # Ya\n",
        "  model_settings = ModelSettings(extra_headers=None)                  # Normal\n",
        "  ```\n",
        "- **Asaan Samajh**: Extra_headers model ko kehta hai ke \"API ko yeh security pass do\". Zyada headers = secure call, kam headers = simple call.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "YicCjAQhIanl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4. raw_response: raw_responses mein agent loop ke dauraan jitni baar LLM ne responses diye, woh saare responses store hote hain, chahe woh ek agent ke ho ya multiple agents ke.**\n",
        "\n",
        "---\n",
        "\n",
        "### **Raw Response Overview**\n",
        "Yeh `raw_response` ek structured output hai jo model ke jawab aur uske metadata (jaise token usage) ko represent karta hai. Iska structure ek list ke andar ek `ModelResponse` object hai, jismein multiple fields hain. Har field ka kaam alag-alag hai. Chalo, ek-ek karke samajhte hain:\n",
        "\n",
        "```python\n",
        "[ModelResponse(\n",
        "    output=[ResponseOutputMessage(\n",
        "        id='__fake_id__',\n",
        "        content=[ResponseOutputText(\n",
        "            annotations=[],\n",
        "            text='Mausam ki jaankaari ke liye, mujhe aapko yeh bataana hoga ki aap kahaan hain. Kripya mujhe apne shehar ya ilaake ka naam bataayein, taaki main aapko sahi jaankaari de sakun.\\n',\n",
        "            type='output_text'\n",
        "        )],\n",
        "        role='assistant',\n",
        "        status='completed',\n",
        "        type='message'\n",
        "    )],\n",
        "    usage=Usage(\n",
        "        requests=1,\n",
        "        input_tokens=19,\n",
        "        input_tokens_details=InputTokensDetails(cached_tokens=0),\n",
        "        output_tokens=61,\n",
        "        output_tokens_details=OutputTokensDetails(reasoning_tokens=0),\n",
        "        total_tokens=80\n",
        "    ),\n",
        "    response_id=None\n",
        ")]\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **1. `ModelResponse`**\n",
        "- **Kya Hai?**: Yeh pura response object hai jo model ke jawab ko represent karta hai. Yeh batata hai ke model ne kya output diya, kitne tokens use hue, aur response ka status kya hai.\n",
        "- **Asaan Alfaaz Mein**: Yeh ek \"dabba\" hai jismein model ka pura jawab aur uski details (jaise kitna kaam kiya) rakhi hain.\n",
        "\n",
        "---\n",
        "\n",
        "### **2. `output` Field**\n",
        "- **Kya Hai?**: Yeh ek list hai jismein model ke actual jawab hote hain. Is case mein, ek hi jawab hai jo `ResponseOutputMessage` object ke roop mein hai.\n",
        "- **Details**:\n",
        "  - `output=[ResponseOutputMessage(...)]`: Yeh batata hai ke model ne ek message diya hai. Agar model multiple messages deta (jaise chat history), to yahan aur items hote.\n",
        "- **Asaan Alfaaz Mein**: Yeh model ka asli jawab hai, jo ek ya zyada messages ka collection ho sakta hai. Yahan sirf ek message hai.\n",
        "\n",
        "---\n",
        "\n",
        "### **3. `ResponseOutputMessage`**\n",
        "- **Kya Hai?**: Yeh ek single message object hai jo model ne generate kiya. Ismein message ki details hoti hain, jaise content, role, aur status.\n",
        "- **Sub-Fields**:\n",
        "  1. **`id='__fake_id__'`**:\n",
        "     - **Kya Hai?**: Yeh message ka unique identifier hai. Yahan `__fake_id__` diya gaya, jo shayad testing ya placeholder ke liye hai.\n",
        "     - **Asaan Alfaaz Mein**: Yeh message ka \"naam\" ya number hai jo usse alag karta hai. Yahan fake ID diya gaya, yani real nahi.\n",
        "  2. **`content=[ResponseOutputText(...)]`**:\n",
        "     - **Kya Hai?**: Yeh message ka actual text ya data hai. Yahan ek `ResponseOutputText` object hai jismein model ka jawab hai.\n",
        "     - **Asaan Alfaaz Mein**: Yeh woh text hai jo model ne bola, jaise \"Mausam ki jaankaari ke liye...\".\n",
        "  3. **`role='assistant'`**:\n",
        "     - **Kya Hai?**: Yeh batata hai ke message kisne diya. `assistant` ka matlab hai ke yeh AI model ka jawab hai (user ke bajaye).\n",
        "     - **Asaan Alfaaz Mein**: Yeh batata hai ke yeh bot ka jawab hai, na ki aapka sawal.\n",
        "  4. **`status='completed'`**:\n",
        "     - **Kya Hai?**: Yeh batata hai ke message ka processing pura ho gaya hai.\n",
        "     - **Asaan Alfaaz Mein**: Iska matlab hai ke model ne apna kaam khatam kar diya, jawab ready hai.\n",
        "  5. **`type='message'`**:\n",
        "     - **Kya Hai?**: Yeh batata hai ke yeh ek message-type output hai (aur doosre types jaise tool calls bhi ho sakte hain).\n",
        "     - **Asaan Alfaaz Mein**: Yeh kehta hai ke yeh ek normal text jawab hai.\n",
        "\n",
        "---\n",
        "\n",
        "### **4. `ResponseOutputText`**\n",
        "- **Kya Hai?**: Yeh message ka actual text content hai, jo model ne generate kiya.\n",
        "- **Sub-Fields**:\n",
        "  1. **`annotations=[]`**:\n",
        "     - **Kya Hai?**: Yeh empty list hai jo text ke saath koi extra information (jaise links, formatting) store kar sakti hai. Yahan koi annotations nahi hain.\n",
        "     - **Asaan Alfaaz Mein**: Yeh text ke saath koi special tags ya notes hote, lekin yahan kuch nahi hai.\n",
        "  2. **`text='Mausam ki jaankaari ke liye...'`**:\n",
        "     - **Kya Hai?**: Yeh model ka actual jawab hai: \"Mausam ki jaankaari ke liye, mujhe aapko yeh bataana hoga ki aap kahaan hain. Kripya mujhe apne shehar ya ilaake ka naam bataayein, taaki main aapko sahi jaankaari de sakun.\"\n",
        "     - **Asaan Alfaaz Mein**: Yeh woh baat hai jo bot ne boli. Model kehta hai ke mausam batane ke liye location chahiye.\n",
        "  3. **`type='output_text'`**:\n",
        "     - **Kya Hai?**: Yeh batata hai ke content ek text message hai (aur doosre types jaise images ya code bhi ho sakte hain).\n",
        "     - **Asaan Alfaaz Mein**: Yeh kehta hai ke yeh jawab text ke roop mein hai.\n",
        "\n",
        "---\n",
        "\n",
        "### **5. `usage` Field**\n",
        "- **Kya Hai?**: Yeh batata hai ke model ne kitne resources (tokens) use kiye request process karne mein.\n",
        "- **Sub-Fields**:\n",
        "  1. **`requests=1`**:\n",
        "     - **Kya Hai?**: Yeh batata hai ke ek request bheja gaya tha.\n",
        "     - **Asaan Alfaaz Mein**: Aapne model ko ek baar sawal poocha.\n",
        "  2. **`input_tokens=19`**:\n",
        "     - **Kya Hai?**: Yeh aapke prompt (\"Mausam ki jaankaari\") ke tokens (words ya subwords) ki ginti hai.\n",
        "     - **Asaan Alfaaz Mein**: Aapke sawal mein 19 \"shabd\" (tokens) the.\n",
        "  3. **`input_tokens_details=InputTokensDetails(cached_tokens=0)`**:\n",
        "     - **Kya Hai?**: Yeh input tokens ke details deta hai. `cached_tokens=0` ka matlab hai ke koi bhi input tokens cached (pehle se save) nahi the.\n",
        "     - **Asaan Alfaaz Mein**: Model ne aapka sawal fresh process kiya, pehle se koi data use nahi kiya.\n",
        "  4. **`output_tokens=61`**:\n",
        "     - **Kya Hai?**: Yeh model ke jawab ke tokens ki ginti hai.\n",
        "     - **Asaan Alfaaz Mein**: Model ke jawab mein 61 \"shabd\" (tokens) hain.\n",
        "  5. **`output_tokens_details=OutputTokensDetails(reasoning_tokens=0)`**:\n",
        "     - **Kya Hai?**: Yeh output tokens ke details deta hai. `reasoning_tokens=0` ka matlab hai ke model ne koi reasoning steps (jaise step-by-step soch) use nahi kiye.\n",
        "     - **Asaan Alfaaz Mein**: Model ne sidha jawab diya, bina kisi extra soch ke.\n",
        "  6. **`total_tokens=80`**:\n",
        "     - **Kya Hai?**: Yeh input aur output tokens ka total (19 + 61 = 80).\n",
        "     - **Asaan Alfaaz Mein**: Total kaam mein 80 \"shabd\" lage.\n",
        "\n",
        "---\n",
        "\n",
        "### **6. `response_id=None`**\n",
        "- **Kya Hai?**: Yeh response ka unique ID hota hai, lekin yahan `None` hai, jo shayad isliye kyunki yeh test ya fake response hai.\n",
        "- **Asaan Alfaaz Mein**: Yeh response ka \"naam\" hota, lekin yahan koi naam nahi diya gaya.\n",
        "\n",
        "---\n",
        "\n",
        "### **Overall Kya Hua?**\n",
        "- **Prompt**: Aapne model se poocha, \"Mausam ki jaankaari.\"\n",
        "- **Jawab**: Model ne kaha, \"Mujhe aapka shehar ya location batao taaki main sahi mausam ki jaankaari de sakun.\"\n",
        "- **Stats**: Aapke sawal mein 19 tokens lage, jawab mein 61 tokens, aur total 80 tokens use hue.\n",
        "- **Status**: Model ne kaam pura kiya (`completed`), aur jawab ek assistant ke roop mein text format mein aaya.\n",
        "- **Annotations**: Koi extra tags ya info nahi di gayi.\n",
        "- **Reasoning**: Model ne koi step-by-step soch ka use nahi kiya (`reasoning_tokens=0`).\n",
        "\n",
        "---\n",
        "\n",
        "### **Asaan Summary**\n",
        "Yeh response ek \"dabba\" hai jismein:\n",
        "1. **Model ka Jawab**: \"Mausam batane ke liye location do.\"\n",
        "2. **Details**: Yeh jawab assistant ne diya, kaam pura ho gaya, aur text format mein hai.\n",
        "3. **Stats**: Sawal mein 19 shabd, jawab mein 61 shabd, total 80 shabd lage.\n",
        "4. **Extra Info**: Koi special tags ya reasoning nahi tha, aur response ka koi unique ID nahi diya gaya.\n",
        "-----------\n"
      ],
      "metadata": {
        "id": "W3xtVUhPQAwl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5. Last_agent:**\n",
        "\n",
        "\n",
        "### **Agent Object Overview**\n",
        "Yeh `Agent` object ek configuration hai jo batata hai ke aapka AI agent (jo ek weather assistant hai) kaise kaam karega. Iska structure complex hai, lekin main ise asaan karke samjhaunga:\n",
        "\n",
        "```python\n",
        "Agent(\n",
        "    name='Assistant',\n",
        "    instructions='You are Powerfull weather Assistant use tool for answering any ewather releated query',\n",
        "    handoff_description=None,\n",
        "    handoffs=[],\n",
        "    model=<agents.models.openai_chatcompletions.OpenAIChatCompletionsModel object at 0x00000218FF7C33D0>,\n",
        "    model_settings=ModelSettings(...),\n",
        "    tools=[FunctionTool(...)],\n",
        "    mcp_servers=[],\n",
        "    mcp_config={},\n",
        "    input_guardrails=[],\n",
        "    output_guardrails=[],\n",
        "    output_type=None,\n",
        "    hooks=None,\n",
        "    tool_use_behavior='run_llm_again',\n",
        "    reset_tool_choice=True\n",
        ")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **1. `name='Assistant'`**\n",
        "- **Kya Hai?**: Yeh agent ka naam hai jo aapne diya hai.\n",
        "- **Asaan Alfaaz Mein**: Yeh batata hai ke is AI ka naam \"Assistant\" hai. Ise aap kuch bhi naam de sakte hain, jaise \"WeatherBot\".\n",
        "\n",
        "---\n",
        "\n",
        "### **2. `instructions='You are Powerfull weather Assistant use tool for answering any ewather releated query'`**\n",
        "- **Kya Hai?**: Yeh agent ko batata hai ke uska kaam kya hai. Yahan instructions kehti hain ke yeh ek \"powerful weather assistant\" hai jo weather-related sawalon ke jawab dene ke liye tools ka use karega.\n",
        "- **Asaan Alfaaz Mein**: Yeh agent ko bolta hai, \"Tum ek weather expert ho, aur jab koi mausam ka sawal puche, to tool use karke jawab do.\" (Note: \"ewather\" shayad typo hai, \"weather\" hona chahiye.)\n",
        "\n",
        "---\n",
        "\n",
        "### **3. `handoff_description=None`**\n",
        "- **Kya Hai?**: Yeh field batata hai ke agar agent kaam na kar sake, to kya doosre agent ko kaam dena hai. Yahan `None` hai, yani koi handoff description nahi diya.\n",
        "- **Asaan Alfaaz Mein**: Yeh kehta hai ke agar yeh agent jawab na de sake, to koi backup plan nahi hai.\n",
        "\n",
        "---\n",
        "\n",
        "### **4. `handoffs=[]`**\n",
        "- **Kya Hai?**: Yeh ek list hai jo doosre agents ke saath handoff rules define karti hai. Yahan empty list (`[]`) hai, yani koi handoff nahi hai.\n",
        "- **Asaan Alfaaz Mein**: Yeh bolta hai ke is agent ke liye koi doosra agent backup ke roop mein nahi hai.\n",
        "\n",
        "---\n",
        "\n",
        "### **5. `model=<agents.models.openai_chatcompletions.OpenAIChatCompletionsModel object at 0x...>`**\n",
        "- **Kya Hai?**: Yeh batata hai ke agent kis AI model ka use kar raha hai. Yahan `OpenAIChatCompletionsModel` hai, jo shayad Gemini API (`gemini-2.0-flash`) ya OpenAI model se juda hai.\n",
        "- **Asaan Alfaaz Mein**: Yeh agent ka \"dimag\" hai, jo sawalon ke jawab deta hai. Is case mein, yeh ek specific model hai jo aapne configure kiya.\n",
        "\n",
        "---\n",
        "\n",
        "### **6. `model_settings=ModelSettings(...)`**\n",
        "- **Kya Hai?**: Yeh `ModelSettings` object hai jo model ke behavior ko control karta hai, jaise temperature, reasoning, metadata, etc. Yahan sab fields `None` hain, yani default settings use ho rahi hain.\n",
        "- **Sub-Fields**:\n",
        "  - `temperature=None`, `top_p=None`, `frequency_penalty=None`, `presence_penalty=None`, etc.: Yeh sab model ke creative ya repetitive behavior ke settings hain. `None` ka matlab hai ke model apne default values use karega.\n",
        "  - `reasoning=None`: Koi step-by-step reasoning nahi hai.\n",
        "  - `metadata=None`: Koi extra info nahi di gayi.\n",
        "  - `store=None`, `include_usage=None`: Response save ya usage stats include nahi kiye.\n",
        "  - `extra_query=None`, `extra_body=None`, `extra_headers=None`: Koi additional API settings nahi hain.\n",
        "- **Asaan Alfaaz Mein**: Yeh settings batati hain ke model kaise jawab dega. Yahan sab default pe chhod diya gaya, yani model apne standard tareeke se kaam karega.\n",
        "\n",
        "---\n",
        "\n",
        "### **7. `tools=[FunctionTool(...)]`**\n",
        "- **Kya Hai?**: Yeh ek list hai jo agent ke paas available tools ko define karti hai. Yahan ek `FunctionTool` hai jo `fetch_weather` ke naam se hai.\n",
        "- **Sub-Fields of `FunctionTool`**:\n",
        "  1. **`name='fetch_weather'`**:\n",
        "     - **Kya Hai?**: Tool ka naam hai, jo weather data fetch karne ke liye hai.\n",
        "     - **Asaan Alfaaz Mein**: Yeh ek \"machine\" hai jo mausam ki jaankaari la sakti hai.\n",
        "  2. **`description=''`**:\n",
        "     - **Kya Hai?**: Tool ka description empty hai, jo batata hai ke tool kya karta hai. Empty hone se model ko thodi confusion ho sakti hai.\n",
        "     - **Asaan Alfaaz Mein**: Yeh tool ke baare mein koi info nahi deta, jo shayad issue ho sakta hai.\n",
        "  3. **`params_json_schema={'properties': {'city': {'title': 'City', 'type': 'string'}}, 'required': ['city'], ...}`**:\n",
        "     - **Kya Hai?**: Yeh JSON schema batata hai ke `fetch_weather` tool ko ek `city` parameter chahiye, jo ek string hona chahiye.\n",
        "     - **Asaan Alfaaz Mein**: Tool ko kaam karne ke liye city ka naam chahiye, jaise \"Delhi\" ya \"Mumbai\".\n",
        "  4. **`on_invoke_tool=<function ...>`**:\n",
        "     - **Kya Hai?**: Yeh function hai jo tab chalta hai jab tool use hota hai.\n",
        "     - **Asaan Alfaaz Mein**: Yeh batata hai ke tool kaise kaam karega jab model usse call karega.\n",
        "  5. **`strict_json_schema=True`**:\n",
        "     - **Kya Hai?**: Yeh ensure karta hai ke tool ka input strictly JSON schema ke mutabik ho.\n",
        "     - **Asaan Alfaaz Mein**: Yeh kehta hai ke tool ko sirf wahi data do jo schema mein define hai (city ka naam).\n",
        "\n",
        "- **Asaan Alfaaz Mein**: Agent ke paas ek tool hai jo mausam ki jaankaari la sakta hai, lekin usse city ka naam chahiye. Tool ka description khali hai, jo model ke liye thodi dikkat paida kar sakta hai.\n",
        "\n",
        "---\n",
        "\n",
        "### **8. `mcp_servers=[]`**\n",
        "- **Kya Hai?**: Yeh list model control plane (MCP) servers ko define karti hai, jo agent ke behavior ko manage karte hain. Yahan empty hai.\n",
        "- **Asaan Alfaaz Mein**: Yeh kehta hai ke koi external server agent ko control nahi kar raha.\n",
        "\n",
        "---\n",
        "\n",
        "### **9. `mcp_config={}`**\n",
        "- **Kya Hai?**: Yeh model control plane ke settings hain. Empty dictionary ka matlab hai ke koi special config nahi hai.\n",
        "- **Asaan Alfaaz Mein**: Agent ke liye koi extra settings nahi hain.\n",
        "\n",
        "---\n",
        "\n",
        "### **10. `input_guardrails=[]`**\n",
        "- **Kya Hai?**: Yeh list input ke liye guardrails (rules) define karti hai, jo check karte hain ke user ka input valid hai ya nahi. Empty hone ka matlab hai ke koi guardrails nahi hain.\n",
        "- **Asaan Alfaaz Mein**: Agent user ke sawal ko check nahi karta, seedha accept kar lega.\n",
        "\n",
        "---\n",
        "\n",
        "### **11. `output_guardrails=[]`**\n",
        "- **Kya Hai?**: Yeh list output ke liye guardrails define karti hai, jo model ke jawab ko validate karte hain. Empty hone ka matlab hai ke koi output check nahi hai.\n",
        "- **Asaan Alfaaz Mein**: Agent jo jawab dega, usse koi rule check nahi karega.\n",
        "\n",
        "---\n",
        "\n",
        "### **12. `output_type=None`**\n",
        "- **Kya Hai?**: Yeh batata hai ke model ka output kaisa hona chahiye (jaise structured dictionary ya plain text). `None` ka matlab hai ke koi specific output type set nahi kiya.\n",
        "- **Asaan Alfaaz Mein**: Agent ka jawab kisi fixed format mein nahi hoga, jo model decide karega.\n",
        "\n",
        "---\n",
        "\n",
        "### **13. `hooks=None`**\n",
        "- **Kya Hai?**: Yeh custom functions ya hooks hote hain jo agent ke kaam ke darmiyan chal sakte hain. `None` ka matlab hai ke koi hooks nahi hain.\n",
        "- **Asaan Alfaaz Mein**: Agent ke kaam mein koi extra steps nahi jode gaye.\n",
        "\n",
        "---\n",
        "\n",
        "### **14. `tool_use_behavior='run_llm_again'`**\n",
        "- **Kya Hai?**: Yeh batata hai ke tool use karne ke baad kya karna hai. `'run_llm_again'` ka matlab hai ke tool ka result milne ke baad model dobara chalega taake final jawab de sake.\n",
        "- **Asaan Alfaaz Mein**: Agar tool (fetch_weather) use hota hai, to model uske result ke saath dobara sochta hai aur jawab deta hai.\n",
        "\n",
        "---\n",
        "\n",
        "### **15. `reset_tool_choice=True`**\n",
        "- **Kya Hai?**: Yeh batata hai ke har request ke baad tool choice reset karna hai ya nahi. `True` ka matlab hai ke har baar model naye tareeke se tool chunega.\n",
        "- **Asaan Alfaaz Mein**: Har sawal ke liye model naye sira se decide karega ke tool use karna hai ya nahi.\n",
        "\n",
        "---\n",
        "\n",
        "### **Overall Kya Hai?**\n",
        "Yeh `Agent` object ek weather assistant ka configuration hai, jo:\n",
        "- **Naam**: \"Assistant\" hai.\n",
        "- **Kaam**: Mausam ke sawalon ke jawab deta hai aur `fetch_weather` tool ka use karta hai.\n",
        "- **Tool**: Ek tool hai jo city ke naam ke saath mausam ki jaankaari la sakta hai.\n",
        "- **Settings**: Sab default pe hain (temperature, reasoning, metadata, etc., sab `None`).\n",
        "- **Behavior**: Tool use karne ke baad model dobara chalega aur har request ke liye tool choice reset hoga.\n",
        "- **Guardrails**: Koi input ya output checks nahi hain.\n",
        "- **Output**: Koi fixed format nahi hai, model jo dega woh accept hoga.\n",
        "\n",
        "---\n",
        "\n",
        "### **Asaan Summary**\n",
        "Yeh ek weather assistant ka \"blueprint\" hai:\n",
        "- **Naam aur Kaam**: Yeh \"Assistant\" hai jo mausam ke sawalon ke liye tool use karta hai.\n",
        "- **Tool**: Ek tool hai jo city ke naam se mausam batata hai.\n",
        "- **Settings**: Sab default pe hain, koi special configuration nahi.\n",
        "- **Rules**: Koi input/output checks nahi, aur tool use ke baad model dobara chalega.\n",
        "- **Output**: Jawab ka format model pe depend karta hai.\n",
        "\n",
        "--------\n"
      ],
      "metadata": {
        "id": "tvD8SYrAgkgB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **6. Turn(action) Confusion:**\n",
        "\n",
        "Aapka sawal OpenAI-Agents SDK ke context mein `max_turns` parameter aur agent ke loop ke behavior se related hai. Specifically, aap janana chahte hain ke jab agent ek loop mein baar-baar LLM (Language Model) ko call karta hai, to kya yeh LLM requests bhi `max_turns` mein count hote hain ya nahi. Main isko asaan alfaaz mein aur SDK ke behavior ke hisaab se explain karunga.\n",
        "\n",
        "---\n",
        "\n",
        "### **Short Answer**\n",
        "OpenAI-Agents SDK mein `max_turns` parameter **agent ke high-level actions** (ya \"turns\") ko count karta hai, na ki har LLM request ko. Yani, agar agent ek turn ke andar multiple LLM calls karta hai (jaise tool use ya reasoning ke liye), to yeh sab ek hi turn mein count hote hain, jab tak agent ka action complete na ho ya naya turn start na ho.\n",
        "\n",
        "---\n",
        "\n",
        "### **Detailed Explanation**\n",
        "\n",
        "#### **1. `max_turns` Kya Hai?**\n",
        "- OpenAI-Agents SDK mein `max_turns` ek parameter hai jo `RunConfig` mein set kiya jata hai. Yeh batata hai ke agent kitne **turns** (ya steps) tak loop chala sakta hai apne task ko complete karne ke liye.\n",
        "- Ek \"turn\" typically ek complete cycle hota hai jismein agent:\n",
        "  1. User input ya context leta hai.\n",
        "  2. LLM ko call karta hai (ek ya zyada baar).\n",
        "  3. Tools ka use karta hai (agar zaroori ho).\n",
        "  4. Ek interim ya final output deta hai.\n",
        "- `max_turns` ka limit isliye hota hai taake agent infinite loop mein na fase.\n",
        "\n",
        "#### **2. Turn vs. LLM Request**\n",
        "- **Turn**: Ek turn ek logical step hai jismein agent apna kaam aage badhata hai. Yeh include kar sakta hai:\n",
        "  - Ek ya zyada LLM calls.\n",
        "  - Tool invocations (jaise `fetch_weather`).\n",
        "  - Internal reasoning ya decision-making.\n",
        "- **LLM Request**: Yeh ek single API call hai jo model (jaise Gemini ya OpenAI) ke server ko jata hai. Ek turn ke andar multiple LLM requests ho sakte hain, jaise:\n",
        "  - Pehle LLM se prompt ka jawab lena.\n",
        "  - Tool use ka decision lene ke liye dobara LLM call.\n",
        "  - Tool ke result ke baad final output ke liye ek aur LLM call.\n",
        "\n",
        "- **Key Point**: SDK mein `max_turns` **turns** ko count karta hai, na ki har LLM request ko. Yani, agar ek turn mein agent 3 LLM calls karta hai (jaise tool call, reasoning, aur final output ke liye), to yeh sab ek hi turn mein count hoga.\n",
        "\n",
        "#### **3. Agent Loop Ka Behavior**\n",
        "OpenAI-Agents SDK mein agent ka loop aise kaam karta hai:\n",
        "1. **Input Processing**: Agent user input ya previous context ko process karta hai.\n",
        "2. **LLM Call**: Agent LLM ko call karta hai jawab ya action ke liye.\n",
        "3. **Tool Use (Optional)**: Agar LLM tool use ka suggestion deta hai (jaise `fetch_weather`), to agent tool ko invoke karta hai aur shayad dobara LLM call karta hai tool ke result ko process karne ke liye.\n",
        "4. **Output**: Jab turn complete hota hai (ya to final jawab mil jata hai ya interim state update hota hai), to agent agle turn pe jata hai ya ruk jata hai.\n",
        "\n",
        "- **Example**:\n",
        "  - Aapka prompt: \"Delhi ka mausam kya hai?\"\n",
        "  - **Turn 1**:\n",
        "    - Agent LLM ko call karta hai aur model kehta hai, \"Mujhe city ke liye tool use karna hoga.\"\n",
        "    - Agent `fetch_weather` tool ko call karta hai (ek aur LLM call ho sakta hai tool ke input ke liye).\n",
        "    - Tool se result aata hai, aur agent ek final LLM call karta hai jawab format karne ke liye: \"Delhi mein aaj 30°C hai.\"\n",
        "    - Yeh sab **ek turn** mein hota hai.\n",
        "  - Agar agent ko aur kaam karna ho (jaise user bolta hai, \"Aur details do\"), to yeh **Turn 2** hoga.\n",
        "  - `max_turns=5` ka matlab hai ke agent aise 5 cycles (turns) tak chal sakta hai.\n",
        "\n",
        "- **LLM Calls in Turn**: Ek turn mein multiple LLM calls ho sakte hain (jaise tool use ke liye, reasoning ke liye, ya output formatting ke liye), lekin yeh sab ek hi turn ke hisaab se count hote hain.\n",
        "\n",
        "#### **4. SDK Ke Hisaab Se**\n",
        "OpenAI-Agents SDK ke GitHub repo (https://github.com/openai/openai-agents-python) aur documentation ke mutabik:\n",
        "- `max_turns` ka control `Runner.run` method mein hota hai, jo agent ke loop ko manage karta hai.\n",
        "- Har turn mein `RunContext` update hota hai, aur turn tab complete hota hai jab agent ek logical step pura karta hai (jaise tool result ke baad jawab dena).\n",
        "- LLM requests ka count `usage` field mein track hota hai (jaise `input_tokens`, `output_tokens`), lekin yeh `max_turns` se alag hai.\n",
        "- Agar aapka agent `tool_use_behavior='run_llm_again'` set karta hai (jo aapke pichle code mein tha), to tool use ke baad LLM dobara call hota hai, lekin yeh bhi usi turn ka hissa hota hai.\n",
        "\n",
        "#### **5. Aapke Sawal Ka Jawab**\n",
        "> Agar agent baar-baar LLM call kar raha hai, to kya LLM requests bhi turn mein count hoti hain?\n",
        "- **Nahi**, LLM requests alag se `max_turns` mein count nahi hoti. Ek turn ke andar kitne bhi LLM calls ho, woh sab usi turn ke andar aate hain. `max_turns` sirf high-level steps (turns) ko limit karta hai, jo agent ke logical actions (jaise input process, tool use, output generate) ke hisaab se count hote hain.\n",
        "- **Example**:\n",
        "  - Agar agent ek turn mein 3 LLM calls karta hai (prompt ke liye, tool call ke liye, aur final jawab ke liye), to yeh **ek turn** count hoga.\n",
        "  - Agar agent 5 turns chalta hai aur har turn mein 2 LLM calls hote hain, to total 10 LLM calls honge, lekin `max_turns=5` ke hisaab se sirf 5 turns count honge.\n",
        "\n",
        "---\n",
        "\n",
        "### **How to Verify?**\n",
        "Agar aap confirm karna chahte hain ke `max_turns` kaise count hota hai:\n",
        "1. **Set `max_turns`**:\n",
        "   ```python\n",
        "   config = RunConfig(\n",
        "       model=model,\n",
        "       model_provider=external_client,\n",
        "       tracing_disabled=True,\n",
        "       max_turns=2  # Limit to 2 turns\n",
        "   )\n",
        "   ```\n",
        "2. **Logging Enable Karen**:\n",
        "   ```python\n",
        "   import logging\n",
        "   logging.basicConfig(level=logging.DEBUG)\n",
        "   ```\n",
        "   - Yeh SDK ke internal logs dikhayega, jismein har turn aur LLM call ka detail hoga.\n",
        "3. **Usage Check Karen**:\n",
        "   - Response mein `usage` field se total LLM requests aur tokens check karen:\n",
        "     ```python\n",
        "     print(result.usage)\n",
        "     ```\n",
        "   - Yeh batayega kitne LLM calls hue, lekin `max_turns` se alag rahega.\n",
        "\n",
        "---\n",
        "\n",
        "### **Asaan Summary**\n",
        "- **Turn**: Ek turn ek pura step hai jismein agent input leta hai, LLM call(s) karta hai, tools use karta hai, aur output deta hai.\n",
        "- **LLM Request**: Ek single API call hai jo model ko jata hai. Ek turn mein multiple LLM calls ho sakte hain.\n",
        "- **Max Turns**: `max_turns` sirf turns ko count karta hai, na ki LLM requests ko. Yani, ek turn ke andar kitne bhi LLM calls ho, woh ek hi turn mein aate hain.\n",
        "- **Aapke Case Mein**: Agar aapka agent loop mein baar-baar LLM call kar raha hai, to yeh calls usi turn ka hissa hain aur `max_turns` mein alag se count nahi hote.\n",
        "\n",
        "-----------"
      ],
      "metadata": {
        "id": "nzVbdWNBk9MC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# **Max_Turn understanding in this code\n",
        "```python\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "import asyncio\n",
        "from pydantic import BaseModel\n",
        "from agents import (\n",
        "    Agent,\n",
        "    AsyncOpenAI,\n",
        "    OpenAIChatCompletionsModel,\n",
        "    RunConfig,\n",
        "    Runner,\n",
        "    ModelSettings,\n",
        "    function_tool,\n",
        ")\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
        "base_url = os.getenv(\"BASE_URL\")\n",
        "\n",
        "external_client = AsyncOpenAI(\n",
        "    api_key=GEMINI_API_KEY,\n",
        "    base_url=\"https://generativelanguage.googleapis.com/v1beta/\",  # Corrected base_url\n",
        ")\n",
        "\n",
        "model = OpenAIChatCompletionsModel(\n",
        "    model=\"gemini-2.0-flash\",\n",
        "    openai_client=external_client,\n",
        ")\n",
        "\n",
        "config = RunConfig(\n",
        "    model=model,\n",
        "    model_provider=external_client,\n",
        "    tracing_disabled=True,\n",
        ")\n",
        "\n",
        "@function_tool\n",
        "def fetch_weather(city: str) -> str:\n",
        "    return f\"Weather of {city} is Cloudy and Temperature is 25C\"\n",
        "\n",
        "weather_agent = Agent(\n",
        "    name=\"Weather_Agent\",\n",
        "    instructions=\"You are a Weather Assistant. Use the fetch_weather tool to answer weather-related queries.\",\n",
        "    model=model,\n",
        "    tools=[fetch_weather],\n",
        ")\n",
        "\n",
        "agent = Agent(\n",
        "    name=\"Assistant\",\n",
        "    instructions=\"You are a Powerful Assistant. Handoff weather-related queries to Weather_Agent.\",\n",
        "    model=model,\n",
        "    handoffs=[weather_agent]\n",
        ")\n",
        "\n",
        "async def main():\n",
        "    result = Runner.run_streamed(\n",
        "        agent,\n",
        "        \"What's the weather in Gujar Khan\",\n",
        "        run_config=config,\n",
        "        max_turns=2\n",
        "    )\n",
        "    async for event in result.stream_events():\n",
        "        print(event)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    asyncio.run(main())\n",
        "```\n",
        ":\n",
        "### **Error Ka Breakdown**\n",
        "- **Error**: `MaxTurnsExceeded` error tab aata hai jab agent `max_turns` se zyada turns (logical steps) leta hai. Aapne `max_turns=2` set kiya hai, lekin agent 2 se zyada turns le raha hai.\n",
        "- **Aapka Sawal**: Aapko lagta hai ke agent sirf 2 turns le raha hai (main agent ka handoff aur weather agent ka tool use), lekin error kyun aa raha hai?\n",
        "- **Problem**: Agent ka workflow aapke soch se zyada complex hai, aur har action (handoff, tool use, final output) alag-alag turns mein count ho raha hai.\n",
        "\n",
        "---\n",
        "\n",
        "### **Agent Kitne Turns Le Raha Hai?**\n",
        "Chalo, aapke code ke flow ko analyze karte hain aur dekhte hain ke agent kitne turns le raha hai jab query hai: \"What's the weather in Gujar Khan\".\n",
        "\n",
        "#### **Code Flow**\n",
        "1. **Main Agent**:\n",
        "   - Name: `Assistant`\n",
        "   - Instructions: \"Handoff weather-related queries to Weather_Agent.\"\n",
        "   - Handoff: `weather_agent`\n",
        "   - Kaam: Query ko process karta hai aur weather-related hone pe `Weather_Agent` ko handoff karta hai.\n",
        "\n",
        "2. **Weather Agent**:\n",
        "   - Name: `Weather_Agent`\n",
        "   - Instructions: \"Use the fetch_weather tool to answer weather-related queries.\"\n",
        "   - Tool: `fetch_weather`\n",
        "   - Kaam: `fetch_weather` tool ko call karta hai aur result return karta hai.\n",
        "\n",
        "3. **Run Configuration**:\n",
        "   - `max_turns=2`: Agent ke total turns ka limit 2 hai.\n",
        "   - `Runner.run_streamed`: Streaming mode mein events generate karta hai.\n",
        "\n",
        "4. **Query**: \"What's the weather in Gujar Khan\"\n",
        "   - Main agent is query ko process karega, handoff karega, aur weather agent tool use karke jawab dega.\n",
        "\n",
        "#### **Turn-by-Turn Breakdown**\n",
        "OpenAI-Agents SDK mein har **turn** ek logical step hai jismein agent input process karta hai, LLM call karta hai, tools use karta hai, ya handoff karta hai. Chalo, turn-by-turn dekhte hain:\n",
        "\n",
        "1. **Turn 1: Main Agent (Handoff Decision)**:\n",
        "   - Main agent query (\"What's the weather in Gujar Khan\") ko process karta hai.\n",
        "   - Instructions ke mutabik, yeh weather-related query hai, isliye agent `Weather_Agent` ko handoff karta hai.\n",
        "   - Is process mein:\n",
        "     - LLM call hota hai query ko analyze karne ke liye.\n",
        "     - Handoff decision liya jata hai.\n",
        "   - **Output**: Shayad ek intermediate output jaise \"Handoff to Weather_Agent\".\n",
        "   - **Turn Count**: Yeh **1st turn** hai.\n",
        "\n",
        "2. **Turn 2: Weather Agent (Tool Use Decision)**:\n",
        "   - `Weather_Agent` handoff se query context leta hai.\n",
        "   - Instructions ke mutabik, yeh `fetch_weather` tool use karta hai.\n",
        "   - Is process mein:\n",
        "     - LLM call hota hai tool use ka decision lene ke liye (e.g., \"fetch_weather(city='Gujar Khan')\").\n",
        "   - **Output**: Tool call ka decision (lekin abhi tool result nahi aaya).\n",
        "   - **Turn Count**: Yeh **2nd turn** hai.\n",
        "\n",
        "3. **Turn 3: Weather Agent (Tool Result Processing)**:\n",
        "   - `fetch_weather` tool call hota hai aur result return karta hai: \"Weather of Gujar Khan is Cloudy and Temperature is 25C\".\n",
        "   - Weather agent is tool result ko process karta hai aur final output generate karta hai.\n",
        "   - Is process mein:\n",
        "     - LLM call hota hai tool result ko format karne ke liye (e.g., final jawab banane ke liye).\n",
        "   - **Output**: Final output jaise \"Weather of Gujar Khan is Cloudy and Temperature is 25C\".\n",
        "   - **Turn Count**: Yeh **3rd turn** hai.\n",
        "\n",
        "#### **Kyun Error Aaya?**\n",
        "- Aapne `max_turns=2` set kiya, lekin agent **3 turns** le raha hai:\n",
        "  - Turn 1: Main agent ka handoff.\n",
        "  - Turn 2: Weather agent ka tool use decision.\n",
        "  - Turn 3: Weather agent ka tool result processing.\n",
        "- Jab agent 3rd turn shuru karta hai, SDK check karta hai ke `max_turns=2` exceed ho gaya, aur `MaxTurnsExceeded` error throw karta hai.\n",
        "\n",
        "---\n",
        "\n",
        "### **Kyun Aapko Laga Ke 2 Turns Hain?**\n",
        "Aapko shayad laga ke:\n",
        "- Turn 1: Main agent ka handoff.\n",
        "- Turn 2: Weather agent ka tool use aur final output.\n",
        "Lekin OpenAI-Agents SDK mein **tool use ka process** do turns mein split ho sakta hai:\n",
        "- Ek turn tool call decision ke liye (LLM se tool choose karna).\n",
        "- Doosra turn tool result ko process aur final output banane ke liye.\n",
        "Isliye total 3 turns ban rahe hain.\n",
        "\n",
        "---\n",
        "\n",
        "### **Solution**\n",
        "Error fix karne ke liye, aapko `max_turns` ko badhana hoga taake sare zaroori turns complete ho saken. `max_turns=3` set karna chahiye kyunki aapka workflow 3 turns le raha hai.\n",
        "\n",
        "#### **Updated Code**\n",
        "```python\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "import asyncio\n",
        "from pydantic import BaseModel\n",
        "from agents import (\n",
        "    Agent,\n",
        "    AsyncOpenAI,\n",
        "    OpenAIChatCompletionsModel,\n",
        "    RunConfig,\n",
        "    Runner,\n",
        "    ModelSettings,\n",
        "    function_tool,\n",
        ")\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
        "base_url = os.getenv(\"BASE_URL\")\n",
        "\n",
        "external_client = AsyncOpenAI(\n",
        "    api_key=GEMINI_API_KEY,\n",
        "    base_url=\"https://generativelanguage.googleapis.com/v1beta/\",\n",
        ")\n",
        "\n",
        "model = OpenAIChatCompletionsModel(\n",
        "    model=\"gemini-2.0-flash\",\n",
        "    openai_client=external_client,\n",
        ")\n",
        "\n",
        "config = RunConfig(\n",
        "    model=model,\n",
        "    model_provider=external_client,\n",
        "    tracing_disabled=True,\n",
        ")\n",
        "\n",
        "@function_tool\n",
        "def fetch_weather(city: str) -> str:\n",
        "    return f\"Weather of {city} is Cloudy and Temperature is 25C\"\n",
        "\n",
        "weather_agent = Agent(\n",
        "    name=\"Weather_Agent\",\n",
        "    instructions=\"You are a Weather Assistant. Use the fetch_weather tool to answer weather-related queries.\",\n",
        "    model=model,\n",
        "    tools=[fetch_weather],\n",
        ")\n",
        "\n",
        "agent = Agent(\n",
        "    name=\"Assistant\",\n",
        "    instructions=\"You are a Powerful Assistant. Handoff weather-related queries to Weather_Agent.\",\n",
        "    model=model,\n",
        "    handoffs=[weather_agent]\n",
        ")\n",
        "\n",
        "async def main():\n",
        "    result = Runner.run_streamed(\n",
        "        agent,\n",
        "        \"What's the weather in Gujar Khan\",\n",
        "        run_config=config,\n",
        "        max_turns=3  # Increased to 3 to allow all turns\n",
        "    )\n",
        "    async for event in result.stream_events():\n",
        "        print(f\"Event: {event}\")\n",
        "        # Extract output and final_output for clarity\n",
        "        if hasattr(event, \"output\") and event.output:\n",
        "            print(\"Intermediate Output:\", event.output)\n",
        "            for output in event.output:\n",
        "                if hasattr(output, \"content\"):\n",
        "                    for content in output.content:\n",
        "                        if hasattr(content, \"text\"):\n",
        "                            print(\"Output Text:\", content.text)\n",
        "        if hasattr(event, \"final_output\") and event.final_output:\n",
        "            print(\"Final Output:\", event.final_output)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    asyncio.run(main())\n",
        "```\n",
        "\n",
        "**Changes**:\n",
        "- `max_turns=3` set kiya taake main agent ka handoff (Turn 1), weather agent ka tool decision (Turn 2), aur tool result processing (Turn 3) complete ho saken.\n",
        "- Event processing mein `output` aur `final_output` ko extract kiya taake streaming events ka flow clear ho.\n",
        "- Logging add kiya (`print` statements) taake har turn ka output dekha ja sake.\n",
        "\n",
        "**Expected Output**:\n",
        "```\n",
        "Event: <RunResultStreaming object with output=[ResponseOutputMessage(...text='Handoff to Weather_Agent...')]>\n",
        "Intermediate Output: [ResponseOutputMessage(...text='Handoff to Weather_Agent...')]\n",
        "Output Text: Handoff to Weather_Agent for weather query\n",
        "Event: <RunResultStreaming object with output=[ResponseOutputMessage(...text='Calling fetch_weather tool...')]>\n",
        "Intermediate Output: [ResponseOutputMessage(...text='Calling fetch_weather tool...')]\n",
        "Output Text: Calling fetch_weather tool for Gujar Khan\n",
        "Event: <RunResultStreaming object with final_output='Weather of Gujar Khan is Cloudy and Temperature is 25C'>\n",
        "Intermediate Output: [ResponseOutputMessage(...text='Weather of Gujar Khan is Cloudy and Temperature is 25C')]\n",
        "Output Text: Weather of Gujar Khan is Cloudy and Temperature is 25C\n",
        "Final Output: Weather of Gujar Khan is Cloudy and Temperature is 25C\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Alternative Solution: Optimize Turns**\n",
        "Agar aap turns ko kam karna chahte hain taake `max_turns=2` mein kaam ho, to aapko workflow optimize karna hoga. Lekin aapke current setup mein (handoff + tool use), 3 turns zaroori hain kyunki:\n",
        "- Handoff ek turn leta hai.\n",
        "- Tool decision ek turn leta hai.\n",
        "- Tool result processing ek turn leta hai.\n",
        "\n",
        "Agar aap handoff ke bajaye **single agent** use karen (weather agent directly query le), to shayad turns kam ho saken (2 turns: tool decision + result processing). Example:\n",
        "\n",
        "```python\n",
        "async def main():\n",
        "    result = Runner.run_streamed(\n",
        "        weather_agent,  # Directly use Weather_Agent\n",
        "        \"What's the weather in Gujar Khan\",\n",
        "        run_config=config,\n",
        "        max_turns=2\n",
        "    )\n",
        "    async for event in result.stream_events():\n",
        "        print(f\"Event: {event}\")\n",
        "        if hasattr(event, \"output\") and event.output:\n",
        "            print(\"Intermediate Output:\", event.output)\n",
        "            for output in event.output:\n",
        "                if hasattr(output, \"content\"):\n",
        "                    for content in output.content:\n",
        "                        if hasattr(content, \"text\"):\n",
        "                            print(\"Output Text:\", content.text)\n",
        "        if hasattr(event, \"final_output\") and event.final_output:\n",
        "            print(\"Final Output:\", event.final_output)\n",
        "```\n",
        "\n",
        "**Kyun Kaam Karega?**:\n",
        "- Handoff ka turn khatam ho jayega kyunki `weather_agent` directly query process karega.\n",
        "- Total turns:\n",
        "  - Turn 1: Tool decision (LLM call for `fetch_weather`).\n",
        "  - Turn 2: Tool result processing aur final output.\n",
        "- `max_turns=2` sufficient hoga.\n",
        "\n",
        "**Expected Output**:\n",
        "```\n",
        "Event: <RunResultStreaming object with output=[ResponseOutputMessage(...text='Calling fetch_weather tool...')]>\n",
        "Intermediate Output: [ResponseOutputMessage(...text='Calling fetch_weather tool...')]\n",
        "Output Text: Calling fetch_weather tool for Gujar Khan\n",
        "Event: <RunResultStreaming object with final_output='Weather of Gujar Khan is Cloudy and Temperature is 25C'>\n",
        "Intermediate Output: [ResponseOutputMessage(...text='Weather of Gujar Khan is Cloudy and Temperature is 25C')]\n",
        "Output Text: Weather of Gujar Khan is Cloudy and Temperature is 25C\n",
        "Final Output: Weather of Gujar Khan is Cloudy and Temperature is 25C\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Kyun Error Aaya?**\n",
        "- **Main Reason**: Aapka workflow 3 turns le raha hai (handoff, tool decision, tool result processing), lekin `max_turns=2` set tha, isliye 3rd turn pe `MaxTurnsExceeded` error aaya.\n",
        "- **Aapka Confusion**: Aapko laga ke handoff aur tool use 2 turns mein complete ho jayega, lekin tool use ka process 2 turns (decision + result processing) mein split ho raha hai.\n",
        "- **SDK Behavior**: OpenAI-Agents SDK mein har logical step (handoff, tool call decision, result processing) ek alag turn count hota hai.\n",
        "\n",
        "---\n",
        "\n",
        "### **Additional Checks**\n",
        "1. **SDK Version**:\n",
        "   - Ensure karen ke aap latest OpenAI-Agents SDK use kar rahe hain:\n",
        "     ```bash\n",
        "     pip show openai-agents\n",
        "     ```\n",
        "     Update karen:\n",
        "     ```bash\n",
        "     pip install --upgrade openai-agents\n",
        "     ```\n",
        "\n",
        "2. **Logging**:\n",
        "   - Debugging ke liye logging enable karen:\n",
        "     ```python\n",
        "     import logging\n",
        "     logging.basicConfig(level=logging.DEBUG)\n",
        "     ```\n",
        "   - Yeh har turn, handoff, aur tool call ka detail dikhayega.\n",
        "\n",
        "3. **Base URL Aur API Key**:\n",
        "   - `.env` file check karen:\n",
        "     ```\n",
        "     GEMINI_API_KEY=your_valid_key\n",
        "     BASE_URL=https://generativelanguage.googleapis.com/v1beta/\n",
        "     ```\n",
        "   - Galat API key ya base_url connection issues de sakta hai.\n",
        "\n",
        "4. **Event Structure**:\n",
        "   - Events ke structure ko confirm karen:\n",
        "     ```python\n",
        "     async for event in result.stream_events():\n",
        "         print(dir(event))  # Event ke available properties dekhein\n",
        "     ```\n",
        "\n",
        "---\n",
        "\n",
        "### **Asaan Summary**\n",
        "- **Problem**: `MaxTurnsExceeded` error isliye aaya kyunki agent 3 turns le raha hai (handoff, tool decision, tool result processing), lekin `max_turns=2` set tha.\n",
        "- **Kitne Turns?**:\n",
        "  - Turn 1: Main agent ka handoff.\n",
        "  - Turn 2: Weather agent ka tool decision.\n",
        "  - Turn 3: Weather agent ka tool result processing.\n",
        "- **Solution**:\n",
        "  - `max_turns=3` set karen taake sare turns complete ho.\n",
        "  - Alternative: Handoff skip karen aur `weather_agent` directly use karen, jo 2 turns mein kaam karega.\n",
        "- **Kyun Hua**: Tool use ka process 2 turns mein split hota hai, aur handoff ek extra turn add karta hai.\n",
        "\n",
        "Agar aapko aur koi error aaye ya specific part samajhna hai (jaise turns ka flow ya event structure), to error message ya details share karen! Main aur help karunga.\n",
        "\n",
        "---------\n",
        "### This Code Tree Structure:\n",
        "\n",
        "```python\n",
        "[RunResultStreaming(\n",
        "    input=\"What's the weather in Gujar Khan\",\n",
        "    new_items=[],\n",
        "    raw_responses=[],\n",
        "    final_output=None,\n",
        "    input_guardrail_results=[],\n",
        "    output_guardrail_results=[],\n",
        "    context_wrapper=RunContextWrapper(\n",
        "        context=None,\n",
        "        usage=Usage(\n",
        "            requests=0,\n",
        "            input_tokens=0,\n",
        "            input_tokens_details=InputTokensDetails(cached_tokens=0),\n",
        "            output_tokens=0,\n",
        "            output_tokens_details=OutputTokensDetails(reasoning_tokens=0),\n",
        "            total_tokens=0\n",
        "        )\n",
        "    ),\n",
        "    current_agent=Agent(\n",
        "        name='Assistant',\n",
        "        instructions='You are a Powerful Assistant. Handoff weather-related queries to Weather_Agent.',\n",
        "        handoff_description=None,\n",
        "        handoffs=[\n",
        "            Agent(\n",
        "                name='Weather_Agent',\n",
        "                instructions='You are a Weather Assistant. Use the fetch_weather tool to answer weather-related queries.',\n",
        "                handoff_description=None,\n",
        "                handoffs=[],\n",
        "                model=OpenAIChatCompletionsModel(...),\n",
        "                model_settings=ModelSettings(\n",
        "                    temperature=None,\n",
        "                    top_p=None,\n",
        "                    frequency_penalty=None,\n",
        "                    presence_penalty=None,\n",
        "                    tool_choice=None,\n",
        "                    parallel_tool_calls=None,\n",
        "                    truncation=None,\n",
        "                    max_tokens=None,\n",
        "                    reasoning=None,\n",
        "                    metadata=None,\n",
        "                    store=None,\n",
        "                    include_usage=None,\n",
        "                    extra_query=None,\n",
        "                    extra_body=None,\n",
        "                    extra_headers=None\n",
        "                ),\n",
        "                tools=[\n",
        "                    FunctionTool(\n",
        "                        name='fetch_weather',\n",
        "                        description='',\n",
        "                        params_json_schema={\n",
        "                            'properties': {\n",
        "                                'city': {'title': 'City', 'type': 'string'}\n",
        "                            },\n",
        "                            'required': ['city'],\n",
        "                            'title': 'fetch_weather_args',\n",
        "                            'type': 'object',\n",
        "                            'additionalProperties': False\n",
        "                        },\n",
        "                        on_invoke_tool=<function ...>,\n",
        "                        strict_json_schema=True\n",
        "                    )\n",
        "                ],\n",
        "                mcp_servers=[],\n",
        "                mcp_config={},\n",
        "                input_guardrails=[],\n",
        "                output_guardrails=[],\n",
        "                output_type=None,\n",
        "                hooks=None,\n",
        "                tool_use_behavior='run_llm_again',\n",
        "                reset_tool_choice=True\n",
        "            )\n",
        "        ],\n",
        "        model=OpenAIChatCompletionsModel(...),\n",
        "        model_settings=ModelSettings(\n",
        "            temperature=None,\n",
        "            top_p=None,\n",
        "            frequency_penalty=None,\n",
        "            presence_penalty=None,\n",
        "            tool_choice=None,\n",
        "            parallel_tool_calls=None,\n",
        "            truncation=None,\n",
        "            max_tokens=None,\n",
        "            reasoning=None,\n",
        "            metadata=None,\n",
        "            store=None,\n",
        "            include_usage=None,\n",
        "            extra_query=None,\n",
        "            extra_body=None,\n",
        "            extra_headers=None\n",
        "        ),\n",
        "        tools=[],\n",
        "        mcp_servers=[],\n",
        "        mcp_config={},\n",
        "        input_guardrails=[],\n",
        "        output_guardrails=[],\n",
        "        output_type=None,\n",
        "        hooks=None,\n",
        "        tool_use_behavior='run_llm_again',\n",
        "        reset_tool_choice=True\n",
        "    ),\n",
        "    current_turn=0,\n",
        "    max_turns=1,\n",
        "    is_complete=False\n",
        ")]\n",
        "```"
      ],
      "metadata": {
        "id": "9f1C9f1rzBOo"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nSLS9uUCXGdd"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **7.Understanding events in streaming:**"
      ],
      "metadata": {
        "id": "ALTCjA17LsCl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "import random\n",
        "from agents import Agent, ItemHelpers, Runner, function_tool\n",
        "\n",
        "@function_tool\n",
        "def how_many_jokes() -> int:\n",
        "    return random.randint(1, 10)\n",
        "\n",
        "\n",
        "async def main():\n",
        "    agent = Agent(\n",
        "        name=\"Joker\",\n",
        "        instructions=\"First call the `how_many_jokes` tool, then tell that many jokes.\",\n",
        "        tools=[how_many_jokes],\n",
        "        model=model\n",
        "    )\n",
        "\n",
        "    result = Runner.run_streamed(\n",
        "        agent,\n",
        "        input=\"Hello\",\n",
        "\n",
        "    )\n",
        "\n",
        "    async for event in result.stream_events():\n",
        "      print(event)\n",
        "\n",
        "\n",
        "asyncio.run(main())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "plEplXMlLqls",
        "outputId": "58d71f09-7b3c-403d-c857-4b5404e6ac23"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AgentUpdatedStreamEvent(new_agent=Agent(name='Joker', instructions='First call the `how_many_jokes` tool, then tell that many jokes.', prompt=None, handoff_description=None, handoffs=[], model=<agents.models.openai_chatcompletions.OpenAIChatCompletionsModel object at 0x798805444b90>, model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=None, truncation=None, max_tokens=None, reasoning=None, metadata=None, store=None, include_usage=None, extra_query=None, extra_body=None, extra_headers=None, extra_args=None), tools=[FunctionTool(name='how_many_jokes', description='', params_json_schema={'properties': {}, 'title': 'how_many_jokes_args', 'type': 'object', 'additionalProperties': False, 'required': []}, on_invoke_tool=<function function_tool.<locals>._create_function_tool.<locals>._on_invoke_tool at 0x7988062ec2c0>, strict_json_schema=True, is_enabled=True)], mcp_servers=[], mcp_config={}, input_guardrails=[], output_guardrails=[], output_type=None, hooks=None, tool_use_behavior='run_llm_again', reset_tool_choice=True), type='agent_updated_stream_event')\n",
            "RawResponsesStreamEvent(data=ResponseCreatedEvent(response=Response(id='__fake_id__', created_at=1750590020.2351408, error=None, incomplete_details=None, instructions=None, metadata=None, model='gemini-2.0-flash', object='response', output=[], parallel_tool_calls=False, temperature=None, tool_choice='auto', tools=[], top_p=None, background=None, max_output_tokens=None, previous_response_id=None, prompt=None, reasoning=None, service_tier=None, status=None, text=None, truncation=None, usage=None, user=None), sequence_number=0, type='response.created'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseOutputItemAddedEvent(item=ResponseFunctionToolCall(arguments='{}', call_id='', name='how_many_jokes', type='function_call', id='__fake_id__', status=None), output_index=0, sequence_number=1, type='response.output_item.added'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseFunctionCallArgumentsDeltaEvent(delta='{}', item_id='__fake_id__', output_index=0, sequence_number=2, type='response.function_call_arguments.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseOutputItemDoneEvent(item=ResponseFunctionToolCall(arguments='{}', call_id='', name='how_many_jokes', type='function_call', id='__fake_id__', status=None), output_index=0, sequence_number=3, type='response.output_item.done'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseCompletedEvent(response=Response(id='__fake_id__', created_at=1750590020.2351408, error=None, incomplete_details=None, instructions=None, metadata=None, model='gemini-2.0-flash', object='response', output=[ResponseFunctionToolCall(arguments='{}', call_id='', name='how_many_jokes', type='function_call', id='__fake_id__', status=None)], parallel_tool_calls=False, temperature=None, tool_choice='auto', tools=[], top_p=None, background=None, max_output_tokens=None, previous_response_id=None, prompt=None, reasoning=None, service_tier=None, status=None, text=None, truncation=None, usage=None, user=None), sequence_number=4, type='response.completed'), type='raw_response_event')\n",
            "RunItemStreamEvent(name='tool_called', item=ToolCallItem(agent=Agent(name='Joker', instructions='First call the `how_many_jokes` tool, then tell that many jokes.', prompt=None, handoff_description=None, handoffs=[], model=<agents.models.openai_chatcompletions.OpenAIChatCompletionsModel object at 0x798805444b90>, model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=None, truncation=None, max_tokens=None, reasoning=None, metadata=None, store=None, include_usage=None, extra_query=None, extra_body=None, extra_headers=None, extra_args=None), tools=[FunctionTool(name='how_many_jokes', description='', params_json_schema={'properties': {}, 'title': 'how_many_jokes_args', 'type': 'object', 'additionalProperties': False, 'required': []}, on_invoke_tool=<function function_tool.<locals>._create_function_tool.<locals>._on_invoke_tool at 0x7988062ec2c0>, strict_json_schema=True, is_enabled=True)], mcp_servers=[], mcp_config={}, input_guardrails=[], output_guardrails=[], output_type=None, hooks=None, tool_use_behavior='run_llm_again', reset_tool_choice=True), raw_item=ResponseFunctionToolCall(arguments='{}', call_id='', name='how_many_jokes', type='function_call', id='__fake_id__', status=None), type='tool_call_item'), type='run_item_stream_event')\n",
            "RunItemStreamEvent(name='tool_output', item=ToolCallOutputItem(agent=Agent(name='Joker', instructions='First call the `how_many_jokes` tool, then tell that many jokes.', prompt=None, handoff_description=None, handoffs=[], model=<agents.models.openai_chatcompletions.OpenAIChatCompletionsModel object at 0x798805444b90>, model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=None, truncation=None, max_tokens=None, reasoning=None, metadata=None, store=None, include_usage=None, extra_query=None, extra_body=None, extra_headers=None, extra_args=None), tools=[FunctionTool(name='how_many_jokes', description='', params_json_schema={'properties': {}, 'title': 'how_many_jokes_args', 'type': 'object', 'additionalProperties': False, 'required': []}, on_invoke_tool=<function function_tool.<locals>._create_function_tool.<locals>._on_invoke_tool at 0x7988062ec2c0>, strict_json_schema=True, is_enabled=True)], mcp_servers=[], mcp_config={}, input_guardrails=[], output_guardrails=[], output_type=None, hooks=None, tool_use_behavior='run_llm_again', reset_tool_choice=True), raw_item={'call_id': '', 'output': '5', 'type': 'function_call_output'}, output=5, type='tool_call_output_item'), type='run_item_stream_event')\n",
            "RawResponsesStreamEvent(data=ResponseCreatedEvent(response=Response(id='__fake_id__', created_at=1750590020.6624231, error=None, incomplete_details=None, instructions=None, metadata=None, model='gemini-2.0-flash', object='response', output=[], parallel_tool_calls=False, temperature=None, tool_choice='auto', tools=[], top_p=None, background=None, max_output_tokens=None, previous_response_id=None, prompt=None, reasoning=None, service_tier=None, status=None, text=None, truncation=None, usage=None, user=None), sequence_number=0, type='response.created'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseOutputItemAddedEvent(item=ResponseOutputMessage(id='__fake_id__', content=[], role='assistant', status='in_progress', type='message'), output_index=0, sequence_number=1, type='response.output_item.added'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseContentPartAddedEvent(content_index=0, item_id='__fake_id__', output_index=0, part=ResponseOutputText(annotations=[], text='', type='output_text', logprobs=None), sequence_number=2, type='response.content_part.added'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta='I', item_id='__fake_id__', output_index=0, sequence_number=3, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=\" am programmed to tell 5 jokes.\\n\\n1. Why don't scientists\", item_id='__fake_id__', output_index=0, sequence_number=4, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=' trust atoms? Because they make up everything!\\n2. Parallel lines have so much', item_id='__fake_id__', output_index=0, sequence_number=5, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=' in common. It’s a shame they’ll never meet.\\n3. Why did the scarecrow win an award? Because he was outstanding in his field', item_id='__fake_id__', output_index=0, sequence_number=6, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta='!\\n4. I told my wife she was drawing her eyebrows too high. She seemed surprised.\\n5. What do you call a fish with no eyes', item_id='__fake_id__', output_index=0, sequence_number=7, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta='? Fsh!\\n', item_id='__fake_id__', output_index=0, sequence_number=8, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseContentPartDoneEvent(content_index=0, item_id='__fake_id__', output_index=0, part=ResponseOutputText(annotations=[], text=\"I am programmed to tell 5 jokes.\\n\\n1. Why don't scientists trust atoms? Because they make up everything!\\n2. Parallel lines have so much in common. It’s a shame they’ll never meet.\\n3. Why did the scarecrow win an award? Because he was outstanding in his field!\\n4. I told my wife she was drawing her eyebrows too high. She seemed surprised.\\n5. What do you call a fish with no eyes? Fsh!\\n\", type='output_text', logprobs=None), sequence_number=9, type='response.content_part.done'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseOutputItemDoneEvent(item=ResponseOutputMessage(id='__fake_id__', content=[ResponseOutputText(annotations=[], text=\"I am programmed to tell 5 jokes.\\n\\n1. Why don't scientists trust atoms? Because they make up everything!\\n2. Parallel lines have so much in common. It’s a shame they’ll never meet.\\n3. Why did the scarecrow win an award? Because he was outstanding in his field!\\n4. I told my wife she was drawing her eyebrows too high. She seemed surprised.\\n5. What do you call a fish with no eyes? Fsh!\\n\", type='output_text', logprobs=None)], role='assistant', status='completed', type='message'), output_index=0, sequence_number=10, type='response.output_item.done'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseCompletedEvent(response=Response(id='__fake_id__', created_at=1750590020.6624231, error=None, incomplete_details=None, instructions=None, metadata=None, model='gemini-2.0-flash', object='response', output=[ResponseOutputMessage(id='__fake_id__', content=[ResponseOutputText(annotations=[], text=\"I am programmed to tell 5 jokes.\\n\\n1. Why don't scientists trust atoms? Because they make up everything!\\n2. Parallel lines have so much in common. It’s a shame they’ll never meet.\\n3. Why did the scarecrow win an award? Because he was outstanding in his field!\\n4. I told my wife she was drawing her eyebrows too high. She seemed surprised.\\n5. What do you call a fish with no eyes? Fsh!\\n\", type='output_text', logprobs=None)], role='assistant', status='completed', type='message')], parallel_tool_calls=False, temperature=None, tool_choice='auto', tools=[], top_p=None, background=None, max_output_tokens=None, previous_response_id=None, prompt=None, reasoning=None, service_tier=None, status=None, text=None, truncation=None, usage=None, user=None), sequence_number=11, type='response.completed'), type='raw_response_event')\n",
            "RunItemStreamEvent(name='message_output_created', item=MessageOutputItem(agent=Agent(name='Joker', instructions='First call the `how_many_jokes` tool, then tell that many jokes.', prompt=None, handoff_description=None, handoffs=[], model=<agents.models.openai_chatcompletions.OpenAIChatCompletionsModel object at 0x798805444b90>, model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=None, truncation=None, max_tokens=None, reasoning=None, metadata=None, store=None, include_usage=None, extra_query=None, extra_body=None, extra_headers=None, extra_args=None), tools=[FunctionTool(name='how_many_jokes', description='', params_json_schema={'properties': {}, 'title': 'how_many_jokes_args', 'type': 'object', 'additionalProperties': False, 'required': []}, on_invoke_tool=<function function_tool.<locals>._create_function_tool.<locals>._on_invoke_tool at 0x7988062ec2c0>, strict_json_schema=True, is_enabled=True)], mcp_servers=[], mcp_config={}, input_guardrails=[], output_guardrails=[], output_type=None, hooks=None, tool_use_behavior='run_llm_again', reset_tool_choice=True), raw_item=ResponseOutputMessage(id='__fake_id__', content=[ResponseOutputText(annotations=[], text=\"I am programmed to tell 5 jokes.\\n\\n1. Why don't scientists trust atoms? Because they make up everything!\\n2. Parallel lines have so much in common. It’s a shame they’ll never meet.\\n3. Why did the scarecrow win an award? Because he was outstanding in his field!\\n4. I told my wife she was drawing her eyebrows too high. She seemed surprised.\\n5. What do you call a fish with no eyes? Fsh!\\n\", type='output_text', logprobs=None)], role='assistant', status='completed', type='message'), type='message_output_item'), type='run_item_stream_event')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "### **1. `AgentUpdatedStreamEvent`**\n",
        "- **Class**: `AgentUpdatedStreamEvent`\n",
        "- **Details**: Yeh event ek naye agent ko introduce karta hai, jiska naam 'Joker' hai aur instructions hain \"You are a helpful assistant.\" Ismein model (OpenAIChatCompletionsModel) aur settings bhi shamil hain.\n",
        "- **Kaam**: Yeh batata hai ke kaunsa agent aapki query ko handle karega aur uski configuration kya hai.\n",
        "- **Purpose**: Process ka starting point set karta hai, taaki system ko pata chale ke kaunsa assistant kaam karega.\n",
        "- **Asaan Alfaaz Mein**: \"System ne 'Joker' naam ka assistant chuna hai jo aapki madad karega. Yeh ab kaam shuru karne ke liye tayyar hai.\"\n",
        "\n",
        "---\n",
        "\n",
        "### **2. `RawResponsesStreamEvent` (ResponseCreatedEvent)**\n",
        "- **Class**: `RawResponsesStreamEvent` jismein `ResponseCreatedEvent` hai.\n",
        "- **Details**: Ismein ek naya response shuru hota hai, jismein model 'gemini-2.0-flash' use ho raha hai, aur basic details jaise ID aur timestamp hain.\n",
        "- **Kaam**: Yeh ek naye response ki shuruaat ko mark karta hai.\n",
        "- **Purpose**: System ko batata hai ke response generate karna shuru ho gaya hai.\n",
        "- **Asaan Alfaaz Mein**: \"System ne aapke sawal ka jawab banana shuru kar diya hai, aur yeh 'gemini-2.0-flash' model se banega.\"\n",
        "\n",
        "---\n",
        "\n",
        "### **3. `RawResponsesStreamEvent` (ResponseOutputItemAddedEvent)**\n",
        "- **Class**: `RawResponsesStreamEvent` jismein `ResponseOutputItemAddedEvent` hai.\n",
        "- **Details**: Ek naya output item (message) add hota hai jo assistant se aayega, aur yeh abhi \"in_progress\" mein hai.\n",
        "- **Kaam**: Response mein assistant ka message shuru karta hai.\n",
        "- **Purpose**: Yeh batata hai ke message ban raha hai, lekin abhi pura nahi hua.\n",
        "- **Asaan Alfaaz Mein**: \"Assistant ne aapke liye message likhna shuru kiya hai, lekin abhi woh adhoora hai.\"\n",
        "\n",
        "---\n",
        "\n",
        "### **4. `RawResponsesStreamEvent` (ResponseContentPartAddedEvent)**\n",
        "- **Class**: `RawResponsesStreamEvent` jismein `ResponseContentPartAddedEvent` hai.\n",
        "- **Details**: Message mein ek content part add hota hai, jo ek text part hai, lekin abhi khali hai.\n",
        "- **Kaam**: Message ke ek section ko shuru karta hai jismein text aayega.\n",
        "- **Purpose**: Yeh text ka placeholder banata hai jo aage bhara jayega.\n",
        "- **Asaan Alfaaz Mein**: \"Message ka ek hissa banaya gaya hai jismein text aayega, lekin abhi woh khali hai.\"\n",
        "\n",
        "---\n",
        "\n",
        "### **5. `RawResponsesStreamEvent` (ResponseTextDeltaEvent)** *(Multiple Instances)*\n",
        "- **Class**: `RawResponsesStreamEvent` jismein `ResponseTextDeltaEvent` hai.\n",
        "- **Details**: Yeh event kai baar aata hai, har baar message mein thoda text add karta hai, jaise:\n",
        "  - \"Alright\"\n",
        "  - \", here are 5 jokes for you:\\n\\n1.  Why don'\"\n",
        "  - \"t scientists trust atoms?\\n    Because they make up everything!\\n\\n2.\"\n",
        "  - Aur aage bhi.\n",
        "- **Kaam**: Message ka text chhote-chhote hisson mein incrementally add hota hai.\n",
        "- **Purpose**: Real-time mein response ko build karta hai, taaki aapko jawab banta hua dikhe.\n",
        "- **Asaan Alfaaz Mein**: \"Assistant message ko thoda-thoda likh raha hai, jaise pehle 'Alright', phir 'here are 5 jokes', aur dheere-dheere pura text ban raha hai.\"\n",
        "\n",
        "---\n",
        "\n",
        "### **6. `RawResponsesStreamEvent` (ResponseContentPartDoneEvent)**\n",
        "- **Class**: `RawResponsesStreamEvent` jismein `ResponseContentPartDoneEvent` hai.\n",
        "- **Details**: Yeh event batata hai ke message ka text part pura ho gaya hai, aur final text hai: \"Alright, here are 5 jokes for you...\" (5 jokes ke saath).\n",
        "- **Kaam**: Message ke ek content part ko complete karta hai.\n",
        "- **Purpose**: Yeh confirm karta hai ke text ka yeh hissa ab final hai.\n",
        "- **Asaan Alfaaz Mein**: \"Message ka text hissa pura ho gaya hai, aur ab yeh 5 jokes ke saath ready hai.\"\n",
        "\n",
        "---\n",
        "\n",
        "### **7. `RawResponsesStreamEvent` (ResponseOutputItemDoneEvent)**\n",
        "- **Class**: `RawResponsesStreamEvent` jismein `ResponseOutputItemDoneEvent` hai.\n",
        "- **Details**: Assistant ka pura message complete ho gaya hai (status: \"completed\"), aur yeh 5 jokes wala text hai.\n",
        "- **Kaam**: Response ka output item (message) complete karta hai.\n",
        "- **Purpose**: Yeh batata hai ke assistant ka message ab final aur tayyar hai.\n",
        "- **Asaan Alfaaz Mein**: \"Assistant ka pura message ban gaya hai, aur ab yeh aapko dikhane ke liye ready hai.\"\n",
        "\n",
        "---\n",
        "\n",
        "### **8. `RawResponsesStreamEvent` (ResponseCompletedEvent)**\n",
        "- **Class**: `RawResponsesStreamEvent` jismein `ResponseCompletedEvent` hai.\n",
        "- **Details**: Pura response complete ho gaya hai, jismein final message (5 jokes) aur details jaise model aur ID shamil hain.\n",
        "- **Kaam**: Pura response process khatam karta hai.\n",
        "- **Purpose**: Yeh confirm karta hai ke jawab final hai aur aapko deliver ho sakta hai.\n",
        "- **Asaan Alfaaz Mein**: \"Aapka pura jawab tayyar ho gaya hai, aur ab system ne ise complete mark kar diya hai.\"\n",
        "\n",
        "---\n",
        "\n",
        "## **9. `RunItemStreamEvent`:**\n",
        "\n",
        "### **Saare Events Ka Chhota Overview**\n",
        "Yeh events do tarah ke hain:\n",
        "1. **`RawResponsesStreamEvent`**: Yeh chhote-chhote steps ko track karte hain, jaise response shuru hona, tool call add hona, ya text ka ek hissa likha jana. Yeh zyada technical aur detailed hote hain.\n",
        "2. **`RunItemStreamEvent`**: Yeh bade aur important steps ko dikhate hain, jaise tool call hona, tool ka result milna, ya final message ban jana. Yeh user-friendly hote hain aur agent ke kaam ka flow samajhne mein madad karte hain.\n",
        "\n",
        "Aapke data mein teeno `RunItemStreamEvent` hain, aur main inko aapke liye explain karunga. Pehle yeh samajh lete hain ke pura process kya hai, phir `RunItemStreamEvent` par gaur karenge.\n",
        "\n",
        "---\n",
        "\n",
        "### **`RunItemStreamEvent` Kya Hai?**\n",
        "Ab specifically `RunItemStreamEvent` ko samajhte hain. Yeh event agent ke bade actions ko track karta hai, taaki aapko pata chale ke assistant ne kya kiya aur kya result mila. Yeh teen baar aata hai aapke data mein, aur har baar ka alag matlab hai:\n",
        "\n",
        "#### **1. `RunItemStreamEvent` (name='tool_called')**\n",
        "- **Details**: Yeh batata hai ke 'Joker' agent ne `how_many_jokes` tool ko call kiya hai.\n",
        "- **Kaam**: Tool ko chalane ka action shuru karna.\n",
        "- **Kyun Zaroori Hai**: Yeh dikhata hai ke agent ne tool use karne ka faisla liya aur ab tool kaam karega.\n",
        "- **Asaan Alfaaz Mein**: \"Assistant ne bola, 'Chalo, `how_many_jokes` tool chalate hain!'\"\n",
        "\n",
        "#### **2. `RunItemStreamEvent` (name='tool_output')**\n",
        "- **Details**: Yeh batata hai ke `how_many_jokes` tool ne apna result diya hai, jo '5' hai (yani 5 jokes batane hain).\n",
        "- **Kaam**: Tool ka output agent ko dena, taaki woh aage kaam kar sake.\n",
        "- **Kyun Zaroori Hai**: Yeh agent ko information deta hai ke kitne jokes batane hain.\n",
        "- **Asaan Alfaaz Mein**: \"Tool ne jawab diya, '5 jokes chahiye.'\"\n",
        "\n",
        "#### **3. `RunItemStreamEvent` (name='message_output_created')**\n",
        "- **Details**: Yeh batata hai ke 'Joker' agent ne final message bana diya hai, jismein 5 jokes hain:\n",
        "  - \"I am programmed to tell 5 jokes.  \n",
        "    1. Why don't scientists trust atoms? Because they make up everything!  \n",
        "    2. Parallel lines have so much in common. It’s a shame they’ll never meet.  \n",
        "    3. Why did the scarecrow win an award? Because he was outstanding in his field!  \n",
        "    4. I told my wife she was drawing her eyebrows too high. She seemed surprised.  \n",
        "    5. What do you call a fish with no eyes? Fsh!\"\n",
        "- **Kaam**: Pura message conversation mein add karna.\n",
        "- **Kyun Zaroori Hai**: Yeh confirm karta hai ke agent ka kaam khatam ho gaya aur jawab aapko mil gaya.\n",
        "- **Asaan Alfaaz Mein**: \"Assistant ne 5 jokes likhkar aapko message de diya.\"\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### **Summary**\n",
        "Yeh saare events ek tarteeb se kaam karte hain:\n",
        "1. **Agent chuna jata hai** (`AgentUpdatedStreamEvent`).\n",
        "2. **Response shuru hota hai** (`ResponseCreatedEvent`).\n",
        "3. **Message banaya jata hai** (`ResponseOutputItemAddedEvent`).\n",
        "4. **Text thoda-thoda add hota hai** (`ResponseTextDeltaEvent`).\n",
        "5. **Text part pura hota hai** (`ResponseContentPartDoneEvent`).\n",
        "6. **Message pura hota hai** (`ResponseOutputItemDoneEvent`).\n",
        "7. **Response complete hota hai** (`ResponseCompletedEvent`).\n",
        "8. **Message conversation mein jata hai** (`RunItemStreamEvent`).\n",
        "\n",
        "Yeh process real-time mein hota hai, jisse aap dekh sakte hain ke assistant kaise jawab bana raha hai—jaise chat mein \"typing...\" dikhayi deta hai. Har event ka apna kaam hota hai, aur saath mein yeh ek smooth streaming response banate hain. Agar aapko kisi event ke baare mein aur detail chahiye, to zaroor batayein!"
      ],
      "metadata": {
        "id": "xdeN3ksCLqRK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "import random\n",
        "from agents import Agent, ItemHelpers, Runner, function_tool\n",
        "\n",
        "@function_tool\n",
        "def how_many_jokes() -> int:\n",
        "    return random.randint(1, 10)\n",
        "\n",
        "\n",
        "async def main():\n",
        "    agent = Agent(\n",
        "        name=\"Joker\",\n",
        "        instructions=\"First call the `how_many_jokes` tool, then tell that many jokes.\",\n",
        "        tools=[how_many_jokes],\n",
        "        model=model\n",
        "    )\n",
        "\n",
        "    result = Runner.run_streamed(\n",
        "        agent,\n",
        "        input=\"Hello\",\n",
        "\n",
        "    )\n",
        "    print(\"=== Run starting ===\")\n",
        "    async for event in result.stream_events():\n",
        "        # We'll ignore the raw responses event deltas\n",
        "        if event.type == \"raw_response_event\":\n",
        "            continue\n",
        "        elif event.type == \"agent_updated_stream_event\":\n",
        "            print(f\"Agent updated: {event.new_agent.name}\")\n",
        "            continue\n",
        "        elif event.type == \"run_item_stream_event\":\n",
        "            if event.item.type == \"tool_call_item\":\n",
        "                print(\"-- Tool was called\")\n",
        "            elif event.item.type == \"tool_call_output_item\":\n",
        "                print(f\"-- Tool output: {event.item.output}\")\n",
        "            elif event.item.type == \"message_output_item\":\n",
        "                text = event.item.raw_item.content[0].text\n",
        "                print(f\"-- Message output:\\n {text}\")\n",
        "                print(f\"-- Message output:\\n {ItemHelpers.text_message_output(event.item)}\")\n",
        "            else:\n",
        "                pass  # Ignore other event types\n",
        "\n",
        "\n",
        "\n",
        "asyncio.run(main())\n",
        "\n",
        "print(\"=== Run complete ===\")\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dBQEuUgQRCvk",
        "outputId": "57897d73-c112-4324-a496-1da761d7e637"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Run starting ===\n",
            "Agent updated: Joker\n",
            "-- Tool was called\n",
            "-- Tool output: 7\n",
            "-- Message output:\n",
            " I am programmed to tell 7 jokes.\n",
            "\n",
            "1. Why don't scientists trust atoms? Because they make up everything!\n",
            "2. Parallel lines have so much in common. It’s a shame they’ll never meet.\n",
            "3. Why did the scarecrow win an award? Because he was outstanding in his field!\n",
            "4. I told my wife she was drawing her eyebrows too high. She seemed surprised.\n",
            "5. What do you call a lazy kangaroo? Pouch potato!\n",
            "6. Why did the bicycle fall over? Because it was two tired!\n",
            "7. I'm reading a book on anti-gravity. It's impossible to put down!\n",
            "\n",
            "=== Run complete ===\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **8: Understanding: {ItemHelpers.text_message_output(event.item)}**\n",
        "\n",
        "----\n",
        "Aapka sawal yeh hai ke `RunItemStreamEvent` mein text kahan hai jise aap access kar rahe hain aur `ItemHelpers.text_message_output` helper ka kya kaam hai. Main ise aapko step-by-step samjhaata hoon taaki sab clear ho jaye.\n",
        "\n",
        "---\n",
        "\n",
        "### **Text Kahan Hai?**\n",
        "Aapke diya hua event ek `RunItemStreamEvent` hai, aur iska structure kuch aisa hai:\n",
        "\n",
        "```python\n",
        "RunItemStreamEvent(\n",
        "    name='message_output_created',\n",
        "    item=MessageOutputItem(\n",
        "        agent=Agent(...),  # Agent ki details\n",
        "        raw_item=ResponseOutputMessage(\n",
        "            id='__fake_id__',\n",
        "            content=[ResponseOutputText(\n",
        "                annotations=[],\n",
        "                text=\"I am programmed to tell 5 jokes.\\n\\n1. Why don't scientists trust atoms? Because they make up everything!\\n2. Parallel lines have so much in common. It’s a shame they’ll never meet.\\n3. Why did the scarecrow win an award? Because he was outstanding in his field!\\n4. I told my wife she was drawing her eyebrows too high. She seemed surprised.\\n5. What do you call a fish with no eyes? Fsh!\\n\",\n",
        "                type='output_text',\n",
        "                logprobs=None\n",
        "            )],\n",
        "            role='assistant',\n",
        "            status='completed',\n",
        "            type='message'\n",
        "        ),\n",
        "        type='message_output_item'\n",
        "    ),\n",
        "    type='run_item_stream_event'\n",
        ")\n",
        "```\n",
        "\n",
        "Is event mein text nikalne ke liye humein structure ke andar jana hoga:\n",
        "\n",
        "1. **`event.item`**: Yeh ek `MessageOutputItem` object hai.\n",
        "2. **`event.item.raw_item`**: Ismein `ResponseOutputMessage` object hai, jo asli message ko hold karta hai.\n",
        "3. **`event.item.raw_item.content`**: Yeh ek list hai jismein `ResponseOutputText` objects hote hain. Is case mein sirf ek object hai.\n",
        "4. **`event.item.raw_item.content[0].text`**: Yahan actual text hai jo aap chahte hain.\n",
        "\n",
        "**Text ki Location**:  \n",
        "Text `event.item.raw_item.content[0].text` mein milta hai. Yani agar aap ise access karna chahein, to aap aise kar sakte hain:\n",
        "\n",
        "```python\n",
        "text = event.item.raw_item.content[0].text\n",
        "print(text)\n",
        "# Output: \"I am programmed to tell 5 jokes.\\n\\n1. Why don't scientists trust atoms? ...\"\n",
        "```\n",
        "\n",
        "Yeh wohi 5 jokes wala message hai jo aapke event mein hai.\n",
        "\n",
        "---\n",
        "\n",
        "### **Helper (`ItemHelpers.text_message_output`) Kya Hai?**\n",
        "Ab baat karte hain aapke code ki is line ki:\n",
        "\n",
        "```python\n",
        "elif event.item.type == \"message_output_item\":\n",
        "    print(f\"-- Message output:\\n {ItemHelpers.text_message_output(event.item)}\")\n",
        "```\n",
        "\n",
        "#### **Helper Ka Kaam**\n",
        "- **`ItemHelpers.text_message_output(event.item)`**: Yeh ek function hai jo `MessageOutputItem` object se text ko extract karta hai aur aapko seedha deta hai.\n",
        "- **Kaise Kaam Karta Hai?**: Yeh shayad `event.item.raw_item.content[0].text` ko return karta hai, ya agar `content` mein multiple text objects hote hain to unko combine karke deta hai.\n",
        "- **Purpose**: Iska maqsad yeh hai ke aapko event ke complex structure mein manually jane ki zarurat na pade. Yeh helper aapka kaam asaan karta hai.\n",
        "\n",
        "**Asaan Alfaaz Mein**:  \n",
        "Helper aapko woh text seedha haath mein de deta hai jo 5 jokes ke saath hai, bina yeh sochne ke ke \"text kahan hai, kaise nikalu?\". For example, agar aap manually `event.item.raw_item.content[0].text` likhte, to bhi kaam ho jata, lekin helper ke saath code chhota aur saaf rehta hai.\n",
        "\n",
        "#### **Helper Ke Fayde**\n",
        "1. **Simplification**: Aapko structure ke andar dhundhne ki zarurat nahi.\n",
        "2. **Readability**: Code zyada clear aur samajhne mein aasan hota hai.\n",
        "3. **Maintainability**: Agar event ka structure future mein badalta hai, to helper us hisaab se adjust ho sakta hai.\n",
        "\n",
        "**Example**:\n",
        "- **Bina Helper Ke**:\n",
        "  ```python\n",
        "  text = event.item.raw_item.content[0].text\n",
        "  print(f\"-- Message output:\\n {text}\")\n",
        "  ```\n",
        "- **Helper Ke Saath**:\n",
        "  ```python\n",
        "  print(f\"-- Message output:\\n {ItemHelpers.text_message_output(event.item)}\")\n",
        "  ```\n",
        "  Dono ka output same hoga, lekin helper wala tariqa zyada clean aur professional hai.\n",
        "\n",
        "---\n",
        "\n",
        "### **Summary**\n",
        "- **Text Kahan Hai?**: `event.item.raw_item.content[0].text` mein. Yeh woh jagah hai jahan 5 jokes ka message store hai.\n",
        "- **Helper Kya Hai?**: `ItemHelpers.text_message_output(event.item)` ek function hai jo text ko event se nikaal ke deta hai, taaki aapko manually structure explore na karna pade.\n",
        "- **Fayda**: Yeh aapke code ko simple, readable, aur future-proof banata hai.\n",
        "\n",
        "-----------"
      ],
      "metadata": {
        "id": "WdKpM-2Jbckl"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BXa6bXS5b9d9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **9. Difference B/W FunctionTool and function_tool:**\n",
        "\n",
        "Aapka sawal OpenAI-Agents SDK mein `FunctionTool` aur `function_tool` ke beech ke farq se related hai, jahan aap dono ko import kar rahe hain:\n",
        "\n",
        "```python\n",
        "from agents import (\n",
        "    function_tool,\n",
        "    FunctionTool\n",
        ")\n",
        "```\n",
        "\n",
        "Main aapko asaan zabaan mein, step-by-step samjhaunga ke yeh dono kya hain, unmein kya farq hai, aur kaise kaam karte hain.\n",
        "\n",
        "---\n",
        "\n",
        "### **1. `FunctionTool` Kya Hai?**\n",
        "- **Kya Hai?**: `FunctionTool` ek **class** hai jo OpenAI-Agents SDK mein define ki gayi hai. Yeh ek tool ko represent karta hai jise agent use kar sakta hai, jaise koi function jo specific kaam karta ho (misal: `fetch_weather`).\n",
        "- **Kaam**: Yeh class ek structured way provide karta hai jismein aap ek tool define karte hain, uske parameters, JSON schema, aur behavior set karte hain.\n",
        "- **Structure**: Ismein fields hote hain jaise:\n",
        "  - `name`: Tool ka naam.\n",
        "  - `description`: Tool kya karta hai.\n",
        "  - `params_json_schema`: Tool ke input parameters ka JSON schema.\n",
        "  - `on_invoke_tool`: Ek callable function jo tool chalane pe run hota hai.\n",
        "  - `strict_json_schema`: Schema validation ke liye boolean.\n",
        "- **Kab Use Hota Hai?**: Jab aap manually ek tool object banate hain ya advanced customization chahiye.\n",
        "\n",
        "**Example**:\n",
        "```python\n",
        "from agents import FunctionTool\n",
        "\n",
        "def my_function(city: str) -> str:\n",
        "    return f\"Weather in {city} is sunny!\"\n",
        "\n",
        "weather_tool = FunctionTool(\n",
        "    name=\"fetch_weather\",\n",
        "    description=\"Fetches weather for a city\",\n",
        "    params_json_schema={\n",
        "        \"properties\": {\"city\": {\"type\": \"string\", \"title\": \"City\"}},\n",
        "        \"required\": [\"city\"],\n",
        "        \"type\": \"object\"\n",
        "    },\n",
        "    on_invoke_tool=my_function,\n",
        "    strict_json_schema=True\n",
        ")\n",
        "```\n",
        "\n",
        "**Asaan Alfaaz Mein**: `FunctionTool` ek blueprint hai jisse aap ek tool banate hain, jismein aap sab kuch manually set karte hain—naam, description, parameters, aur function jo chalega.\n",
        "\n",
        "---\n",
        "\n",
        "### **2. `function_tool` Kya Hai?**\n",
        "- **Kya Hai?**: `function_tool` ek **decorator** function hai jo OpenAI-Agents SDK mein diya gaya hai. Yeh ek Python function ko `FunctionTool` object mein convert karta hai.\n",
        "- **Kaam**: Yeh aapke function ke liye automatically `FunctionTool` banata hai, jismein function ke signature (parameters aur return type) se JSON schema generate hota hai, aur function khud `on_invoke_tool` ke roop mein set hota hai.\n",
        "- **Structure**: Yeh decorator aapke function ko wrap karta hai aur ek `FunctionTool` instance return karta hai. Ismein bhi wahi fields hote hain jo `FunctionTool` class mein hote hain, lekin zyadatar cheezein automatically set ho jati hain.\n",
        "- **Kab Use Hota Hai?**: Jab aap ek simple function ko jaldi se tool mein badalna chahte hain bina manually schema ya settings likhe.\n",
        "\n",
        "**Example**:\n",
        "```python\n",
        "from agents import function_tool\n",
        "\n",
        "@function_tool\n",
        "def fetch_weather(city: str) -> str:\n",
        "    return f\"Weather of {city} is Cloudy and Temperature is 25C\"\n",
        "```\n",
        "\n",
        "- **Kya Hota Hai?**: Yeh decorator `fetch_weather` function ko ek `FunctionTool` object mein convert karta hai. Automatically:\n",
        "  - `name=\"fetch_weather\"` set hota hai.\n",
        "  - `params_json_schema` function ke parameter (`city: str`) se ban jata hai.\n",
        "  - `on_invoke_tool` ko `fetch_weather` function assign hota hai.\n",
        "  - Default settings (jaise `strict_json_schema=True`) apply hoti hain.\n",
        "\n",
        "**Asaan Alfaaz Mein**: `function_tool` ek shortcut hai jo aapke function ko ek tool mein badal deta hai, aur aapko zyada likhne ki zarurat nahi padti.\n",
        "\n",
        "---\n",
        "\n",
        "### **Kaise Decide Karen Kya Use Karna Hai?**\n",
        "- **`FunctionTool` Use Karen Jab**:\n",
        "  - Aapko ek tool ke liye specific JSON schema chahiye jo function ke signature se alag ho.\n",
        "  - Aap non-Python function (jaise external API call) ko tool banane ke liye use karna chahte hain.\n",
        "  - Aapko description ya extra settings (jaise custom validation) add karne hain.\n",
        "  - Example: Agar aap ek API call ko tool banate hain jiska schema function se nahi ban sakta.\n",
        "\n",
        "- **`function_tool` Use Karen Jab**:\n",
        "  - Aapka tool ek simple Python function hai jiska schema function ke parameters se ban sakta hai.\n",
        "  - Aap jaldi aur kam code mein tool banana chahte hain.\n",
        "  - Example: Aapke code mein `fetch_weather` jaisa function jo direct result deta hai.\n",
        "\n",
        "---\n",
        "\n",
        "### **Aapke Code Mein Context**\n",
        "Aapke pichle code mein aapne `function_tool` ka use kiya tha:\n",
        "\n",
        "```python\n",
        "@function_tool\n",
        "def fetch_weather(city: str) -> str:\n",
        "    return f\"Weather of {city} is Cloudy and Temperature is 25C\"\n",
        "```\n",
        "\n",
        "Yahan `function_tool` decorator ne `fetch_weather` ko ek `FunctionTool` object mein convert kiya, jo `Weather_Agent` ke `tools` list mein add hua. Isne automatically:\n",
        "- `name=\"fetch_weather\"` set kiya.\n",
        "- `params_json_schema` banaya (`city` parameter ke liye).\n",
        "- `on_invoke_tool` ko `fetch_weather` function assign kiya.\n",
        "\n",
        "Agar aap yeh manually `FunctionTool` se karte, to zyada code likhna padta:\n",
        "\n",
        "```python\n",
        "weather_tool = FunctionTool(\n",
        "    name=\"fetch_weather\",\n",
        "    params_json_schema={\n",
        "        \"properties\": {\"city\": {\"type\": \"string\", \"title\": \"City\"}},\n",
        "        \"required\": [\"city\"],\n",
        "        \"type\": \"object\",\n",
        "        \"additionalProperties\": False\n",
        "    },\n",
        "    on_invoke_tool=fetch_weather,\n",
        "    strict_json_schema=True\n",
        ")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Technical Details**\n",
        "- **Internal Working**:\n",
        "  - `function_tool` decorator shayad `FunctionTool` class ka ek instance banata hai. Iska source code shayad kuch aisa hoga (pseudocode):\n",
        "    ```python\n",
        "    def function_tool(func):\n",
        "        schema = generate_schema_from_function(func)\n",
        "        return FunctionTool(\n",
        "            name=func.__name__,\n",
        "            params_json_schema=schema,\n",
        "            on_invoke_tool=func,\n",
        "            strict_json_schema=True\n",
        "        )\n",
        "    ```\n",
        "  - Yeh Python ke function signature (type hints jaise `city: str`) se JSON schema banata hai.\n",
        "\n",
        "- **Source Code Reference**:\n",
        "  - OpenAI-Agents SDK ke GitHub repo (https://github.com/openai/openai-agents-python) mein `agents/tools.py` ya similar file mein `FunctionTool` class aur `function_tool` decorator ka implementation mil sakta hai.\n",
        "  - `FunctionTool` ek base class hai jo shayad `Tool` class se inherit karta hai, aur `function_tool` iska wrapper hai.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PgM10T6Oae8N"
      }
    }
  ]
}